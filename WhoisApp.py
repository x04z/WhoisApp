import streamlit as st
from streamlit_option_menu import option_menu
import pandas as pd
import requests
import time
from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED
import socket
import struct
import ipaddress
from urllib.parse import quote
import math
import altair as alt 
import json 
import io 

# --- Excelã‚°ãƒ©ãƒ•ç”Ÿæˆç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.chart import BarChart, Reference, Series
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(layout="wide", page_title="Whois Search Tool", page_icon="ğŸŒ")

# ==========================================
# ğŸ› ï¸ è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ (st.secretsåˆ©ç”¨)
# ==========================================
# ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯ secrets.toml ãŒãªãã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã‚ˆã† try-except ã§å‡¦ç†
# Cloudå´ã§ ENV_MODE = "public" ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ã€æ©Ÿèƒ½åˆ¶é™ãƒ¢ãƒ¼ãƒ‰(True)ã«ãªã‚‹
IS_PUBLIC_MODE = False
try:
    if "ENV_MODE" in st.secrets and st.secrets["ENV_MODE"] == "public":
        IS_PUBLIC_MODE = True
except FileNotFoundError:
    # ãƒ­ãƒ¼ã‚«ãƒ«ã§secretsãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ãŒãªã„å ´åˆã¯å…¨æ©Ÿèƒ½ãƒ¢ãƒ¼ãƒ‰(False)ã¨ã™ã‚‹
    IS_PUBLIC_MODE = False
# ==========================================

# --- è¨­å®š ---
MODE_SETTINGS = {
    "å®‰å®šæ€§é‡è¦– (2.5ç§’å¾…æ©Ÿ/å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰)": {
        "MAX_WORKERS": 1, 
        "DELAY_BETWEEN_REQUESTS": 2.5 
    },
    "é€Ÿåº¦å„ªå…ˆ (1.4ç§’å¾…æ©Ÿ/2ã‚¹ãƒ¬ãƒƒãƒ‰)": {
        "MAX_WORKERS": 2, 
        "DELAY_BETWEEN_REQUESTS": 1.4 
    }
}
IP_API_URL = "http://ip-api.com/json/{ip}?fields=status,country,countryCode,isp,query,message"
RATE_LIMIT_WAIT_SECONDS = 120 
  
RIR_LINKS = {
    'RIPE': 'https://apps.db.ripe.net/db-web-ui/#/query?searchtext={ip}',
    'ARIN': 'https://search.arin.net/rdap/?query={ip}',
    'APNIC': 'https://wq.apnic.net/static/search.html',
    'JPNIC': 'https://www.nic.ad.jp/ja/whois/ja-gateway.html',
    'AFRINIC': 'https://www.afrinic.net/whois',
    'ICANN Whois': 'https://lookup.icann.org/',
}

SECONDARY_TOOL_BASE_LINKS = {
    'VirusTotal': 'https://www.virustotal.com/',
    'Whois.com': 'https://www.whois.com/',
    'Who.is': 'https://who.is/',
    'DomainSearch.jp': 'https://www.domainsearch.jp/',
    'Aguse': 'https://www.aguse.jp/',
    'IP2Proxy': 'https://www.ip2proxy.com/',
    'DNS Checker': 'https://dnschecker.org/',
    'DNSlytics': 'https://dnslytics.com/',
    'IP Location': 'https://iplocation.io/',
    'CP-WHOIS': 'https://doco.cph.jp/whoisweb.php',
    }

COUNTRY_CODE_TO_RIR = {
    'JP': 'JPNIC', 'CN': 'APNIC', 'AU': 'APNIC', 'KR': 'APNIC', 'IN': 'APNIC',
    'ID': 'APNIC', 'MY': 'APNIC', 'NZ': 'APNIC', 'SG': 'APNIC',
    'TH': 'APNIC', 'VN': 'APNIC', 'PH': 'APNIC', 'PK': 'APNIC', 
    'BD': 'APNIC', 'HK': 'APNIC', 'TW': 'APNIC', 'NP': 'APNIC', 'LK': 'APNIC',
    'MO': 'APNIC', 
    'US': 'ARIN', 'CA': 'ARIN',
    'ZA': 'AFRINIC', 'EG': 'AFRINIC', 'NG': 'AFRINIC',
    'KE': 'AFRINIC', 'DZ': 'AFRINIC', 'MA': 'AFRINIC', 'GH': 'AFRINIC', 
    'CM': 'AFRINIC', 'TN': 'AFRINIC', 'ET': 'AFRINIC', 'TZ': 'AFRINIC',
    'DE': 'RIPE', 'FR': 'RIPE', 'GB': 'RIPE', 'RU': 'RIPE',
    'NL': 'RIPE', 'IT': 'RIPE', 'ES': 'RIPE', 'PL': 'RIPE', 
    'TR': 'RIPE', 'UA': 'RIPE', 'SA': 'RIPE', 'IR': 'RIPE', 
    'CH': 'RIPE', 'SE': 'RIPE', 'NO': 'RIPE', 'DK': 'RIPE', 
    'BE': 'RIPE', 'AT': 'RIPE', 'GR': 'RIPE', 'PT': 'RIPE',
    'IE': 'RIPE', 'FI': 'RIPE', 'CZ': 'RIPE', 'RO': 'RIPE',
    'HU': 'RIPE', 'IL': 'RIPE', 'KZ': 'RIPE', 'BG': 'RIPE',
    'HR': 'RIPE', 'RS': 'RIPE', 'AE': 'RIPE', 'QA': 'RIPE',
}

COUNTRY_CODE_TO_NUMERIC_ISO = {
    'AF': 4, 'AL': 8, 'DZ': 12, 'AS': 16, 'AD': 20, 'AO': 24, 'AI': 660, 'AQ': 10, 'AG': 28, 'AR': 32,
    'AM': 51, 'AW': 533, 'AU': 36, 'AT': 40, 'AZ': 31, 'BS': 44, 'BH': 48, 'BD': 50, 'BB': 52, 'BY': 112,
    'BE': 56, 'BZ': 84, 'BJ': 204, 'BM': 60, 'BT': 64, 'BO': 68, 'BA': 70, 'BW': 72, 'BV': 74, 'BR': 76,
    'VG': 92, 'IO': 86, 'BN': 96, 'BG': 100, 'BF': 854, 'BI': 108, 'KH': 116, 'CM': 120, 'CA': 124, 'CV': 132,
    'KY': 136, 'CF': 140, 'TD': 148, 'CL': 152, 'CN': 156, 'CX': 162, 'CC': 166, 'CO': 170, 'KM': 174, 'CG': 178,
    'CD': 180, 'CK': 184, 'CO': 170, 'CR': 188, 'HR': 191, 'CU': 192, 'CY': 196, 'CZ': 203, 'DK': 208, 'DJ': 262, 'DM': 212,
    'DO': 214, 'EC': 218, 'EG': 818, 'SV': 222, 'GQ': 226, 'ER': 232, 'EE': 233, 'ET': 231, 'FK': 238, 'FO': 234,
    'FJ': 242, 'FI': 246, 'FR': 250, 'GF': 254, 'PF': 258, 'TF': 260, 'GA': 266, 'GM': 270, 'GE': 268, 'DE': 276,
    'GH': 288, 'GI': 292, 'GR': 300, 'GL': 304, 'GD': 308, 'GP': 312, 'GU': 316, 'GT': 320, 'GN': 324, 'GW': 624,
    'GY': 328, 'HT': 332, 'HM': 334, 'VA': 336, 'HN': 340, 'HK': 344, 'HU': 348, 'IS': 352, 'IN': 356, 'ID': 360,
    'IR': 364, 'IQ': 368, 'IE': 372, 'IL': 376, 'IT': 380, 'CI': 384, 'JM': 388, 'JP': 392, 'JO': 400, 'KZ': 398,
    'KE': 404, 'KI': 296, 'KP': 408, 'KR': 410, 'KW': 414, 'KG': 417, 'LA': 418, 'LV': 428, 'LB': 422, 'LS': 426,
    'LR': 430, 'LY': 434, 'LI': 438, 'LT': 440, 'LU': 442, 'MO': 446, 'MK': 807, 'MG': 450, 'MW': 454, 'MY': 458,
    'MV': 462, 'ML': 466, 'MT': 470, 'MH': 584, 'MQ': 474, 'MR': 478, 'MU': 480, 'YT': 175, 'MX': 484, 'FM': 583,
    'MD': 498, 'MC': 492, 'MN': 496, 'MS': 500, 'MA': 504, 'MZ': 508, 'MM': 104, 'NA': 516, 'NR': 520, 'NP': 524,
    'NL': 528, 'AN': 530, 'NC': 540, 'NZ': 554, 'NI': 558, 'NE': 562, 'NG': 566, 'NU': 570, 'NF': 574, 'MP': 580,
    'NO': 578, 'OM': 512, 'PK': 586, 'PW': 585, 'PS': 275, 'PA': 591, 'PG': 598, 'PY': 600, 'PE': 604, 'PH': 608,
    'PN': 612, 'PL': 616, 'PT': 620, 'PR': 630, 'QA': 634, 'RE': 638, 'RO': 642, 'RU': 643, 'RW': 646, 'SH': 654,
    'KN': 659, 'LC': 662, 'PM': 666, 'VC': 670, 'WS': 882, 'SM': 674, 'ST': 678, 'SA': 682, 'SN': 686, 'RS': 688,
    'SC': 690, 'SL': 694, 'SG': 702, 'SK': 703, 'SI': 705, 'SB': 90, 'SO': 706, 'ZA': 710, 'GS': 239, 'ES': 724,
    'LK': 144, 'SD': 736, 'SR': 740, 'SJ': 744, 'SZ': 748, 'SE': 752, 'CH': 756, 'SY': 760, 'TW': 158, 'TJ': 762,
    'TZ': 834, 'TH': 764, 'TL': 626, 'TG': 768, 'TK': 772, 'TO': 776, 'TT': 780, 'TN': 788, 'TR': 792, 'TM': 795,
    'TC': 796, 'TV': 798, 'UG': 800, 'UA': 804, 'AE': 784, 'GB': 826, 'US': 840, 'UM': 581, 'UY': 858, 'UZ': 860,
    'VU': 548, 'VE': 862, 'VN': 704, 'VI': 850, 'WF': 876, 'EH': 732, 'YE': 887, 'ZM': 894, 'ZW': 716
}

COUNTRY_JP_NAME = {
    "AF": "ã‚¢ãƒ•ã‚¬ãƒ‹ã‚¹ã‚¿ãƒ³","AL": "ã‚¢ãƒ«ãƒãƒ‹ã‚¢","DZ": "ã‚¢ãƒ«ã‚¸ã‚§ãƒªã‚¢","AS": "ã‚¢ãƒ¡ãƒªã‚«é ˜ã‚µãƒ¢ã‚¢","AD": "ã‚¢ãƒ³ãƒ‰ãƒ©","AO": "ã‚¢ãƒ³ã‚´ãƒ©",
    "AI": "ã‚¢ãƒ³ã‚®ãƒ©","AQ": "å—æ¥µ","AG": "ã‚¢ãƒ³ãƒ†ã‚£ã‚°ã‚¢ãƒ»ãƒãƒ¼ãƒ–ãƒ¼ãƒ€","AR": "ã‚¢ãƒ«ã‚¼ãƒ³ãƒãƒ³","AM": "ã‚¢ãƒ«ãƒ¡ãƒ‹ã‚¢","AW": "ã‚¢ãƒ«ãƒ","AU": "ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢",
    "AT": "ã‚ªãƒ¼ã‚¹ãƒˆãƒªã‚¢","AZ": "ã‚¢ã‚¼ãƒ«ãƒã‚¤ã‚¸ãƒ£ãƒ³","BS": "ãƒãƒãƒ","BH": "ãƒãƒ¼ãƒ¬ãƒ¼ãƒ³","BD": "ãƒãƒ³ã‚°ãƒ©ãƒ‡ã‚·ãƒ¥","BB": "ãƒãƒ«ãƒãƒ‰ã‚¹","BY": "ãƒ™ãƒ©ãƒ«ãƒ¼ã‚·",
    "BE": "ãƒ™ãƒ«ã‚®ãƒ¼","BZ": "ãƒ™ãƒªãƒ¼ã‚º","BJ": "ãƒ™ãƒŠãƒ³","BM": "ãƒãƒŸãƒ¥ãƒ¼ãƒ€","BT": "ãƒ–ãƒ¼ã‚¿ãƒ³","BO": "ãƒœãƒªãƒ“ã‚¢","BA": "ãƒœã‚¹ãƒ‹ã‚¢ãƒ»ãƒ˜ãƒ«ãƒ„ã‚§ã‚´ãƒ“ãƒŠ",
    "BW": "ãƒœãƒ„ãƒ¯ãƒŠ","BR": "ãƒ–ãƒ©ã‚¸ãƒ«","BN": "ãƒ–ãƒ«ãƒã‚¤","BG": "ãƒ–ãƒ«ã‚¬ãƒªã‚¢","BF": "ãƒ–ãƒ«ã‚­ãƒŠãƒ•ã‚¡ã‚½","BI": "ãƒ–ãƒ«ãƒ³ã‚¸","KH": "ã‚«ãƒ³ãƒœã‚¸ã‚¢","CM": "ã‚«ãƒ¡ãƒ«ãƒ¼ãƒ³",
    "CA": "ã‚«ãƒŠãƒ€","CV": "ã‚«ãƒ¼ãƒœãƒ™ãƒ«ãƒ‡","CF": "ä¸­å¤®ã‚¢ãƒ•ãƒªã‚«å…±å’Œå›½","TD": "ãƒãƒ£ãƒ‰","CL": "ãƒãƒª","CN": "ä¸­å›½","CO": "ã‚³ãƒ­ãƒ³ãƒ“ã‚¢","CR": "ã‚³ã‚¹ã‚¿ãƒªã‚«",
    "HR": "ã‚¯ãƒ­ã‚¢ãƒã‚¢","CU": "ã‚­ãƒ¥ãƒ¼ãƒ","CY": "ã‚­ãƒ—ãƒ­ã‚¹","CZ": "ãƒã‚§ã‚³","DK": "ãƒ‡ãƒ³ãƒãƒ¼ã‚¯","DJ": "ã‚¸ãƒ–ãƒ","DM": "ãƒ‰ãƒŸãƒ‹ã‚«å›½","DO": "ãƒ‰ãƒŸãƒ‹ã‚«å…±å’Œå›½",
    "EC": "ã‚¨ã‚¯ã‚¢ãƒ‰ãƒ«","EG": "ã‚¨ã‚¸ãƒ—ãƒˆ","SV": "ã‚¨ãƒ«ã‚µãƒ«ãƒãƒ‰ãƒ«","EE": "ã‚¨ã‚¹ãƒˆãƒ‹ã‚¢","ET": "ã‚¨ãƒã‚ªãƒ”ã‚¢","FI": "ãƒ•ã‚£ãƒ³ãƒ©ãƒ³ãƒ‰","FR": "ãƒ•ãƒ©ãƒ³ã‚¹","DE": "ãƒ‰ã‚¤ãƒ„",
    "GR": "ã‚®ãƒªã‚·ãƒ£","GL": "ã‚°ãƒªãƒ¼ãƒ³ãƒ©ãƒ³ãƒ‰","GT": "ã‚°ã‚¢ãƒ†ãƒãƒ©","GY": "ã‚¬ã‚¤ã‚¢ãƒŠ","HK": "é¦™æ¸¯","HU": "ãƒãƒ³ã‚¬ãƒªãƒ¼","IN": "ã‚¤ãƒ³ãƒ‰","ID": "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢",
    "IR": "ã‚¤ãƒ©ãƒ³","IQ": "ã‚¤ãƒ©ã‚¯","IE": "ã‚¢ã‚¤ãƒ«ãƒ©ãƒ³ãƒ‰","IL": "ã‚¤ã‚¹ãƒ©ã‚¨ãƒ«","IT": "ã‚¤ã‚¿ãƒªã‚¢","JP": "æ—¥æœ¬","KR": "éŸ“å›½","TW": "å°æ¹¾","MY": "ãƒãƒ¬ãƒ¼ã‚·ã‚¢",
    "MX": "ãƒ¡ã‚­ã‚·ã‚³","NL": "ã‚ªãƒ©ãƒ³ãƒ€","NZ": "ãƒ‹ãƒ¥ãƒ¼ã‚¸ãƒ¼ãƒ©ãƒ³ãƒ‰","NO": "ãƒãƒ«ã‚¦ã‚§ãƒ¼","PK": "ãƒ‘ã‚­ã‚¹ã‚¿ãƒ³","PA": "ãƒ‘ãƒŠãƒ","PE": "ãƒšãƒ«ãƒ¼","PH": "ãƒ•ã‚£ãƒªãƒ”ãƒ³",
    "PL": "ãƒãƒ¼ãƒ©ãƒ³ãƒ‰","PT": "ãƒãƒ«ãƒˆã‚¬ãƒ«","QA": "ã‚«ã‚¿ãƒ¼ãƒ«","RO": "ãƒ«ãƒ¼ãƒãƒ‹ã‚¢","RU": "ãƒ­ã‚·ã‚¢","SA": "ã‚µã‚¦ã‚¸ã‚¢ãƒ©ãƒ“ã‚¢","SG": "ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«","ZA": "å—ã‚¢ãƒ•ãƒªã‚«",
    "ES": "ã‚¹ãƒšã‚¤ãƒ³","SE": "ã‚¹ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ³","CH": "ã‚¹ã‚¤ã‚¹","TH": "ã‚¿ã‚¤","TR": "ãƒˆãƒ«ã‚³","UA": "ã‚¦ã‚¯ãƒ©ã‚¤ãƒŠ","AE": "ã‚¢ãƒ©ãƒ–é¦–é•·å›½é€£é‚¦","GB": "ã‚¤ã‚®ãƒªã‚¹",
    "US": "ã‚¢ãƒ¡ãƒªã‚«","VN": "ãƒ™ãƒˆãƒŠãƒ ","YE": "ã‚¤ã‚¨ãƒ¡ãƒ³","ZM": "ã‚¶ãƒ³ãƒ“ã‚¢","ZW": "ã‚¸ãƒ³ãƒãƒ–ã‚¨"
}

# --- ISPåç§°ã®æ—¥æœ¬èªãƒãƒƒãƒ”ãƒ³ã‚° (ä¼æ¥­åçµ±ä¸€ç‰ˆ) ---
ISP_JP_NAME = {
    # --- NTT Group ---
    'NTT Communications Corporation': 'NTTãƒ‰ã‚³ãƒ¢ãƒ“ã‚¸ãƒã‚¹', 
    'NTT COMMUNICATIONS CORPORATION': 'NTTãƒ‰ã‚³ãƒ¢ãƒ“ã‚¸ãƒã‚¹',
    'NTT DOCOMO BUSINESS,Inc.': 'NTTãƒ‰ã‚³ãƒ¢ãƒ“ã‚¸ãƒã‚¹',
    'NTT DOCOMO, INC.': 'NTTãƒ‰ã‚³ãƒ¢',
    'NTT PC Communications, Inc.': 'NTTPCã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚º',
    
    # --- KDDI Group ---
    'Kddi Corporation': 'KDDI',
    'Chubu Telecommunications Co., Inc.': 'ä¸­éƒ¨ãƒ†ãƒ¬ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³',
    'Chubu Telecommunications Company, Inc.': 'ä¸­éƒ¨ãƒ†ãƒ¬ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³',
    'Hokkaido Telecommunication Network Co., Inc.': 'HOTnet',
    'Energia Communications, Inc.': 'ã‚¨ãƒã‚³ãƒ ',
    'STNet, Inc.': 'STNet',
    'QTNet, Inc.': 'QTNet',
    'BIGLOBE Inc.': 'ãƒ“ãƒƒã‚°ãƒ­ãƒ¼ãƒ–',
    
    # --- SoftBank Group ---
    'SoftBank Corp.': 'ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯',
    'Yahoo Japan Corporation': 'LINEãƒ¤ãƒ•ãƒ¼',
    'LY Corporation': 'LINEãƒ¤ãƒ•ãƒ¼',
    'LINE Corporation': 'LINEãƒ¤ãƒ•ãƒ¼',
    
    # --- Rakuten Group ---
    'Rakuten Group, Inc.': 'æ¥½å¤©ã‚°ãƒ«ãƒ¼ãƒ—',
    'Rakuten Mobile, Inc.': 'æ¥½å¤©ãƒ¢ãƒã‚¤ãƒ«',
    'Rakuten Communications Corp.': 'æ¥½å¤©ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚º',
    
    # --- Sony Group ---
    'Sony Network Communications Inc.': 'ã‚½ãƒ‹ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚º',
    'So-net Entertainment Corporation': 'ã‚½ãƒ‹ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚º', 
    'So-net Corporation': 'ã‚½ãƒ‹ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚º',
    
    # --- Major ISPs / VNEs ---
    'Internet Initiative Japan Inc.': 'IIJ',
    'NIFTY Corporation': 'ãƒ‹ãƒ•ãƒ†ã‚£',
    'FreeBit Co., Ltd.': 'ãƒ•ãƒªãƒ¼ãƒ“ãƒƒãƒˆ',
    'TOKAI Communications Corporation': 'TOKAIã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚º',
    'DREAM TRAIN INTERNET INC.': 'ãƒ‰ãƒªãƒ¼ãƒ ãƒ»ãƒˆãƒ¬ã‚¤ãƒ³ãƒ»ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ (DTI)',
    'ASAHI Net, Inc.': 'æœæ—¥ãƒãƒƒãƒˆ',
    'Asahi Net': 'æœæ—¥ãƒãƒƒãƒˆ',
    'Optage Inc.': 'ã‚ªãƒ—ãƒ†ãƒ¼ã‚¸',
    'Jupiter Telecommunications Co., Ltd.': 'J:COM', 
    'JCOM Co., Ltd.': 'J:COM',
    'JCN': 'J:COM', 
    'SAKURA Internet Inc.': 'ã•ãã‚‰ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ',
    'GMO Internet, Inc.': 'GMOã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ',
    'INTERNET MULTIFEED CO.': 'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒãƒ«ãƒãƒ•ã‚£ãƒ¼ãƒ‰',
    'IDC Frontier Inc.': 'IDCãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢',
    
    # --- Others ---
    'ARTERIA Networks Corporation': 'ã‚¢ãƒ«ãƒ†ãƒªã‚¢ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¹',
    'UCOM Corporation': 'ã‚¢ãƒ«ãƒ†ãƒªã‚¢ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¹',
    'VECTANT Ltd.': 'ã‚¢ãƒ«ãƒ†ãƒªã‚¢ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¹',
    'KIBI Cable Television Co., Ltd.': 'å‰å‚™ã‚±ãƒ¼ãƒ–ãƒ«ãƒ†ãƒ¬ãƒ“',
}

# æ­£è¦åŒ–é–¢æ•°: å°æ–‡å­—åŒ–ã—ã€ã‚«ãƒ³ãƒ(,)ã¨ãƒ”ãƒªã‚ªãƒ‰(.)ã‚’é™¤å»ã™ã‚‹
def normalize_isp_key(text):
    if not text:
        return ""
    # å°æ–‡å­—å¤‰æ› -> ã‚«ãƒ³ãƒå‰Šé™¤ -> ãƒ”ãƒªã‚ªãƒ‰å‰Šé™¤ -> å‰å¾Œã®ç©ºç™½å‰Šé™¤
    return text.lower().replace(',', '').replace('.', '').strip()

# æ¤œç´¢ç”¨ã«ã‚­ãƒ¼ã‚’æ­£è¦åŒ–ã—ãŸè¾æ›¸ã‚’ä½œæˆï¼ˆå¤§æ–‡å­—å°æ–‡å­—ãƒ»è¨˜å·ã®æºã‚‰ãã‚’å¸åï¼‰
# ä¾‹: "NTT DOCOMO, INC." -> "ntt docomo inc"
ISP_JP_NAME_NORMALIZED = {normalize_isp_key(k): v for k, v in ISP_JP_NAME.items()}

# --- åŒ¿ååŒ–ãƒ»ãƒ—ãƒ­ã‚­ã‚·åˆ¤å®šç”¨ãƒ‡ãƒ¼ã‚¿ ---

@st.cache_data(ttl=86400, show_spinner=False)
def fetch_tor_exit_nodes():
    terminal = st.empty()
    log_lines = []
    
    def update_log(new_line, color="#00FF41"):
        log_lines.append(f"<span style='color:{color};'>[SYS] {new_line}</span>")
        display_text = "<br>".join(log_lines[-5:])
        terminal.markdown(f"""
            <div style="background-color: rgba(13, 2, 8, 0.9); border: 1px solid #FF0055; padding: 15px; border-radius: 8px; font-family: 'Courier New', Courier, monospace; font-size: 14px; line-height: 1.3; box-shadow: 0 0 20px rgba(255, 0, 85, 0.4); margin-bottom: 20px;">
                <div style="color: #FF0055; font-weight: bold; margin-bottom: 5px; font-size: 10px; border-bottom: 1px solid #FF0055;">ENCRYPTED DATA STREAMING...</div>
                {display_text}
            </div>
        """, unsafe_allow_html=True)
        time.sleep(0.3)

    update_log("BOOTING NEURAL LINK...")
    update_log("DECRYPTING EXIT NODE MANIFEST...")
    
    try:
        url = "https://check.torproject.org/exit-addresses"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        update_log("HANDSHAKE SUCCESSFUL.", "#00FFFF")
        
        exit_ips = set()
        for line in response.text.splitlines():
            if line.startswith("ExitAddress"):
                parts = line.split()
                if len(parts) >= 2:
                    exit_ips.add(parts[1])
        
        update_log(f"NODES LOADED: {len(exit_ips)} UNITS", "#00FFFF")
        update_log("SESSION SECURED. SYSTEM ONLINE.", "#00FF41")
        time.sleep(1.0)
        terminal.empty()
        return exit_ips
        
    except Exception as e:
        update_log(f"CRITICAL ERROR: {e}", "#FF0000")
        time.sleep(2.0)
        terminal.empty()
        return set()

HOSTING_VPN_KEYWORDS = [
    "hosting", "datacenter", "vps", "cloud", "server", "vpn", "proxy", "dedi",
    "amazon technologies", "amazon.com", "google llc", "google cloud", "microsoft corporation", "azure",
    "oracle cloud", "alibaba", "tencent", "huawei", "digitalocean", "linode", "vultr", "ovh", "hetzner",
    "m247", "proweb", "choopa", "leaseweb", "datacamp", "ip-volume", "flyservers", 
    "performive", "hostroyale", "packet exchange", "xtom", "tzulo", "psychz", 
    "franantech", "buyvm", "melbicom", "pfcloud", "epyc", "layerhost",
    "akamai", "cloudflare", "fastly", "cdn77", "imperva", "incapsula", "cloudfront",
    "expressvpn", "nordvpn", "proton", "mullvad", "private internet access", "windscribe",
    "cyberghost", "torguard", "vyprvpn", "purevpn"
]

def detect_proxy_vpn_tor(ip, isp_name, tor_nodes):
    isp_lower = isp_name.lower()
    if ip in tor_nodes: return "Tor Node"
    if "icloud" in isp_lower or "private relay" in isp_lower: return "iCloud Private Relay"
    privacy_keywords = ["vpn", "proxy", "applied privacy", "privacy foundation", "calyx institute", "foundation for applied privacy"]
    if any(kw in isp_lower for kw in privacy_keywords): return "VPN/Proxy (Named)"
    if any(kw in isp_lower for kw in HOSTING_VPN_KEYWORDS):
        if any(cdn in isp_lower for cdn in ["cloudflare", "akamai", "fastly", "cloudfront"]): return "CDN/Proxy"
        return "Hosting/DataCenter"
    # ä¿®æ­£: Residential/Business -> Standard Connection
    return "Standard Connection"

def get_jp_names(english_isp, country_code):
    if not english_isp:
        return "N/A", COUNTRY_JP_NAME.get(country_code, country_code)

    # 1. ã¾ãšå®Œå…¨ä¸€è‡´ã‚’è©¦ã™ (åŸºæœ¬)
    if english_isp in ISP_JP_NAME:
        jp_isp = ISP_JP_NAME[english_isp]
    else:
        # 2. æ­£è¦åŒ–ã—ã¦æ¤œç´¢ (ã‚«ãƒ³ãƒãƒ»ãƒ”ãƒªã‚ªãƒ‰ãƒ»å¤§å°æ–‡å­—ã‚’ç„¡è¦–ã—ã¦ç…§åˆ)
        normalized_input = normalize_isp_key(english_isp)
        jp_isp = ISP_JP_NAME_NORMALIZED.get(normalized_input, english_isp)

    jp_country = COUNTRY_JP_NAME.get(country_code, country_code)
    return jp_isp, jp_country

@st.cache_resource
def get_session():
    session = requests.Session()
    session.headers.update({"User-Agent": "WhoisBatchTool/1.4 (+PythonStreamlitApp)"})
    return session

session = get_session()

@st.cache_data
def get_world_map_data():
    try:
        world_geojson = alt.topo_feature('https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/world-110m.json', 'countries')
        return world_geojson
    except Exception as e:
        st.error(f"GeoJSONãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

WORLD_MAP_GEOJSON = get_world_map_data()


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ ---
def clean_ocr_error_chars(target):
    cleaned_target = target
    cleaned_target = cleaned_target.replace('â…¡', '11')
    cleaned_target = cleaned_target.replace('I', '1')
    cleaned_target = cleaned_target.replace('l', '1')
    cleaned_target = cleaned_target.replace('|', '1')
    cleaned_target = cleaned_target.replace('O', '0')
    cleaned_target = cleaned_target.replace('o', '0')
    cleaned_target = cleaned_target.replace('S', '5')
    cleaned_target = cleaned_target.replace('s', '5')
    if ':' not in cleaned_target:
        cleaned_target = cleaned_target.replace('A', '4')
        cleaned_target = cleaned_target.replace('a', '4')
        cleaned_target = cleaned_target.replace('B', '8')
    return cleaned_target

def is_valid_ip(target):
    try:
        ipaddress.ip_address(target)
        return True
    except ValueError:
        return False

def is_ipv4(target):
    try:
        ipaddress.IPv4Address(target)
        return True
    except ValueError:
        return False

def ip_to_int(ip):
    try:
        if is_ipv4(ip):
            return struct.unpack("!I", socket.inet_aton(ip))[0]
        return 0
    except OSError:
        return 0

def get_cidr_block(ip, netmask_range=(8, 24)):
    try:
        ip_obj = ipaddress.ip_address(ip)
        if ip_obj.version == 4:
            netmask = netmask_range[1] 
            network = ipaddress.ip_network(f'{ip}/{netmask}', strict=False)
            return str(network)
        elif ip_obj.version == 6:
            netmask = 48
            network = ipaddress.ip_network(f'{ip}/{netmask}', strict=False)
            return str(network)
        return None
    except ValueError:
        return None

def get_authoritative_rir_link(ip, country_code):
    rir_name = COUNTRY_CODE_TO_RIR.get(country_code)
    if rir_name and rir_name in RIR_LINKS:
        encoded_ip = quote(ip, safe='')
        if rir_name in ['RIPE', 'ARIN']:
            link_url = RIR_LINKS[rir_name].format(ip=encoded_ip)
            return f"[{rir_name}]({link_url})"
        elif rir_name in ['JPNIC', 'APNIC', 'LACNIC', 'AFRINIC']:
            link_url = RIR_LINKS[rir_name]  
            return f"[{rir_name} (æ‰‹å‹•æ¤œç´¢)]({link_url})"
    return f"[Whois (æ±ç”¨æ¤œç´¢ - APNICçª“å£)]({RIR_LINKS.get('APNIC', 'https://wq.apnic.net/static/search.html')})"

def get_copy_target(ip_display):
    if not ip_display: return ""
    return str(ip_display).split(' - ')[0].split(' ')[0]

def create_secondary_links(target):
    encoded_target = quote(target, safe='')
    is_ip = is_valid_ip(target)
    is_ipv6 = is_ip and not is_ipv4(target)

    who_is_url = f'https://who.is/whois-ip/ip-address/{encoded_target}' if is_ip else f'https://who.is/whois/{encoded_target}'
    dns_checker_url = ''
    dns_checker_key = ''

    if is_ip:
        dns_checker_path = 'ipv6-whois-lookup.php' if is_ipv6 else 'ip-whois-lookup.php'
        dns_checker_url = f'https://dnschecker.org/{dns_checker_path}?query={encoded_target}'
        dns_checker_key = 'DNS Checker (æ‰‹å‹• - IPv6)' if is_ipv6 else 'DNS Checker'
    else:
        dns_checker_url = f'https://dnschecker.org/whois-lookup.php?query={encoded_target}'
        dns_checker_key = 'DNS Checker (ãƒ‰ãƒ¡ã‚¤ãƒ³)'

    all_links = {
        'VirusTotal': f'https://www.virustotal.com/gui/search/{encoded_target}',
        'Aguse': f'https://www.aguse.jp/?url={encoded_target}',
        'Whois.com': f'https://www.whois.com/whois/{encoded_target}',
        'DomainSearch.jp': f'https://www.domainsearch.jp/whois/?q={encoded_target}',
        'Who.is': who_is_url,
        'IP2Proxy': f'https://www.ip2proxy.com/{encoded_target}',
        'DNSlytics (æ‰‹å‹•)': 'https://dnslytics.com/whois-lookup/',
        'IP Location': f'https://iplocation.io/ip/{encoded_target}',
        'CP-WHOIS (æ‰‹å‹•)': 'https://doco.cph.jp/whoisweb.php',
    }

    if dns_checker_url:
        all_links[dns_checker_key] = dns_checker_url

    if is_ipv6:
        links = {
            'VirusTotal': all_links['VirusTotal'],
            'DomainSearch.jp': all_links['DomainSearch.jp'],
            dns_checker_key: all_links[dns_checker_key],
            'IP2Proxy': all_links['IP2Proxy'],
            'DNSlytics (æ‰‹å‹•)': all_links['DNSlytics (æ‰‹å‹•)'],
            'IP Location': all_links['IP Location'],
            'CP-WHOIS (æ‰‹å‹•)': all_links['CP-WHOIS (æ‰‹å‹•)'],
        }
    else:
        links = all_links

    link_html = ""
    for name, url in links.items():
        link_html += f"[{name}]({url}) | "
    return link_html.rstrip(' | ')


# --- APIé€šä¿¡é–¢æ•° ---
def get_ip_details_from_api(ip, cidr_cache_snapshot, delay_between_requests, rate_limit_wait_seconds, tor_nodes):
    result = {
        'Target_IP': ip, 'ISP': 'N/A', 'ISP_JP': 'N/A', 'Country': 'N/A', 'Country_JP': 'N/A', 
        'CountryCode': 'N/A', 'RIR_Link': 'N/A', 'Secondary_Security_Links': 'N/A', 'Status': 'N/A'
    }
    new_cache_entry = None

    cidr_block = get_cidr_block(ip)
    
    if cidr_block and cidr_block in cidr_cache_snapshot:
        cached_data = cidr_cache_snapshot[cidr_block]
        if time.time() - cached_data['Timestamp'] < 86400:
            result['ISP'] = cached_data['ISP']
            result['Country'] = cached_data['Country']
            result['CountryCode'] = cached_data['CountryCode']
            result['Status'] = "Success (Cache)" 
            jp_isp, jp_country = get_jp_names(result['ISP'], result['CountryCode'])
            proxy_type = detect_proxy_vpn_tor(ip, result['ISP'], tor_nodes)
            is_anonymous = (proxy_type != "Standard Connection")
            result['ISP_JP'] = jp_isp
            result['Proxy_Type'] = f"{proxy_type}" if is_anonymous else ""
            result['Country_JP'] = jp_country
            return result, None 

    try:
        time.sleep(delay_between_requests) 

        url = IP_API_URL.format(ip=ip)
        response = session.get(url, timeout=45)
        
        if response.status_code == 429:
            defer_until = time.time() + rate_limit_wait_seconds
            result['Status'] = 'Error: Rate Limit (429)'
            result['Defer_Until'] = defer_until
            result['Secondary_Security_Links'] = create_secondary_links(ip)
            return result, new_cache_entry 
        
        response.raise_for_status()
        data = response.json()
        
        if data.get('status') == 'success':
            country_code = data.get('countryCode', 'N/A') 

            result['ISP'] = data.get('isp', 'N/A')
            result['Country'] = data.get('country', 'N/A')
            result['CountryCode'] = data.get('countryCode', 'N/A')
            result['RIR_Link'] = get_authoritative_rir_link(ip, country_code)
            status_type = "IPv6 API" if not is_ipv4(ip) else "IPv4 API"
            result['Status'] = f'Success ({status_type})'
            jp_isp, jp_country = get_jp_names(result['ISP'], country_code)
            proxy_type = detect_proxy_vpn_tor(ip, result['ISP'], tor_nodes)
            is_anonymous = (proxy_type != "Standard Connection")
            result['ISP_JP'] = jp_isp
            result['Proxy_Type'] = f"{proxy_type}" if is_anonymous else ""
            result['Country_JP'] = jp_country
            
            if cidr_block:
                new_cache_entry = {
                    cidr_block: {
                    'ISP': result['ISP'],
                    'Country': result['Country'],
                    'CountryCode': result['CountryCode'],
                    'Timestamp': time.time()
                    }
                }
            
        elif data.get('status') == 'fail':
            result['Status'] = f"API Fail: {data.get('message', 'Unknown Fail')}"
            result['RIR_Link'] = get_authoritative_rir_link(ip, 'N/A')
            
        else:
            result['Status'] = f"API Error: Unknown Response"
            result['RIR_Link'] = get_authoritative_rir_link(ip, 'N/A')
            
    except requests.exceptions.RequestException as e:
        result['Status'] = f'Error: Network/Timeout ({type(e).__name__})'
        
    result['Secondary_Security_Links'] = create_secondary_links(ip)
    return result, new_cache_entry

def get_domain_details(domain):
    icann_link = f"[ICANN Whois (æ‰‹å‹•æ¤œç´¢)]({RIR_LINKS['ICANN Whois']})"
    return {
        'Target_IP': domain, 'ISP': 'Domain/Host', 'Country': 'N/A', 'CountryCode': 'N/A',
        'RIR_Link': icann_link,
        'Secondary_Security_Links': create_secondary_links(domain),
        'Status': 'Success (Domain)'
    }

def get_simple_mode_details(target):
    if is_valid_ip(target):
        rir_link_content = f"[Whois (æ±ç”¨æ¤œç´¢ - APNICçª“å£)]({RIR_LINKS['APNIC']})"
    else:
        rir_link_content = f"[ICANN Whois (æ‰‹å‹•æ¤œç´¢)]({RIR_LINKS['ICANN Whois']})"
        
    return {
        'Target_IP': target, 
        'ISP': 'N/A (ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰)', 
        'Country': 'N/A (ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰)',
        'CountryCode': 'N/A',
        'RIR_Link': rir_link_content,
        'Secondary_Security_Links': create_secondary_links(target),
        'Status': 'Success (ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰)' 
    }

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ ---

def group_results_by_isp(results):
    grouped = {}
    final_grouped_results = []
    non_aggregated_results = []
    successful_results = [res for res in results if res['Status'].startswith('Success')]

    for res in successful_results:
        is_ip = is_valid_ip(res['Target_IP'])
        if not is_ip or not is_ipv4(res['Target_IP']) or res['ISP'] == 'N/A' or res['Country'] == 'N/A' or res['ISP'] == 'N/A (ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰)':
            if res['Status'].startswith('Success (IPv4 CIDR Cache)'):
                non_aggregated_results.append(res)
            else:
                non_aggregated_results.append(res)
            continue
        
        key = (res['ISP'], res['CountryCode']) 
        
        if key not in grouped:
            grouped[key] = {
                'IP_Ints': [], 'IPs_List': [], 'RIR_Link': res['RIR_Link'],
                'Secondary_Security_Links': res['Secondary_Security_Links'],
                'ISP': res['ISP'], 
                'Country': res['Country'], 
                'Status': res['Status'],
                'ISP_JP': res.get('ISP_JP', 'N/A'),
                'Country_JP': res.get('Country_JP', 'N/A')
            }
        ip_int = ip_to_int(res['Target_IP'])
        if ip_int != 0:
            grouped[key]['IP_Ints'].append(ip_int)
            grouped[key]['IPs_List'].append(res['Target_IP'])
        else:
            res['Status'] = 'Error: IPv4 Int Conversion Failed'
            non_aggregated_results.append(res)

    non_aggregated_results.extend([res for res in results if not res['Status'].startswith('Success')])
    
    for key, data in grouped.items():
        if not data['IP_Ints']: 
            continue
            
        sorted_ip_ints = sorted(data['IP_Ints'])
        min_int = sorted_ip_ints[0]
        max_int = sorted_ip_ints[-1]
        count = len(data['IPs_List'])
        try:
            min_ip = str(ipaddress.IPv4Address(min_int))
            max_ip = str(ipaddress.IPv4Address(max_int))
        except ValueError:
            min_ip = data['IPs_List'][0]
            max_ip = data['IPs_List'][-1]
        
        target_ip_display = min_ip if count == 1 else f"{min_ip} - {max_ip} (x{count} IPs)"
        status_display = data['Status'] if count == 1 else f"Aggregated ({count} IPs)"
        
        final_grouped_results.append({
            'Target_IP': target_ip_display, 
            'Country': data['Country'], 
            'Country_JP': data['Country_JP'], 
            'ISP': data['ISP'],
            'ISP_JP': data['ISP_JP'], 
            'RIR_Link': data['RIR_Link'], 
            'Secondary_Security_Links': data['Secondary_Security_Links'],
            'Status': status_display
        })
    
    final_grouped_results.extend(non_aggregated_results)

    return final_grouped_results

# --- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆé–¢æ•° ---
def summarize_in_realtime(raw_results):
    isp_counts = {}
    country_counts = {}
    country_code_counts = {}

    target_frequency = st.session_state.get('target_freq_map', {})

    st.session_state['debug_summary'] = {} 

    country_all_df_raw = pd.DataFrame({
        'NumericCode': pd.Series(dtype='int64'), 
        'Count': pd.Series(dtype='int64'),
        'Country': pd.Series(dtype='str')
    })

    success_ipv4 = [
        r for r in raw_results 
        if r['Status'].startswith('Success') and is_ipv4(r['Target_IP'])
    ]

    for r in success_ipv4:
        ip = r.get('Target_IP')
        frequency = target_frequency.get(ip, 1) 

        isp_name = r.get('ISP_JP', r.get('ISP', 'N/A'))
        country_name = r.get('Country_JP', r.get('Country', 'N/A'))
        cc = r.get('CountryCode', 'N/A')
        
        if isp_name and isp_name not in ['N/A', 'N/A (ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰)']:
            isp_counts[isp_name] = isp_counts.get(isp_name, 0) + frequency
        
        if country_name and country_name != 'N/A':
            country_counts[country_name] = country_counts.get(country_name, 0) + frequency
            
        if cc and cc != 'N/A':
            country_code_counts[cc] = country_code_counts.get(cc, 0) + frequency

    # --- ISPé›†è¨ˆ ---
    isp_full_df = pd.DataFrame(list(isp_counts.items()), columns=['ISP', 'Count'])
    isp_full_df = isp_full_df.sort_values('Count', ascending=False)
    
    if not isp_full_df.empty:
        isp_df = isp_full_df.head(10).copy()
        isp_df['ISP'] = isp_df['ISP'].str.wrap(25)
    else:
        isp_df = pd.DataFrame(columns=['ISP', 'Count'])

    # --- å›½é›†è¨ˆ ---
    country_full_df = pd.DataFrame(list(country_counts.items()), columns=['Country', 'Count'])
    country_full_df = country_full_df.sort_values('Count', ascending=False)

    if not country_full_df.empty:
        country_df = country_full_df.head(10).copy()
        country_df['Country'] = country_df['Country'].str.wrap(25)
    else:
        country_df = pd.DataFrame(columns=['Country', 'Count'])

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨
    if country_code_counts:
        map_data = []
        for cc, cnt in country_code_counts.items():
            num = COUNTRY_CODE_TO_NUMERIC_ISO.get(cc)
            if num is not None:
                name_for_map = COUNTRY_JP_NAME.get(cc, cc)
                map_data.append({
                    'NumericCode': int(num), 
                    'Count': int(cnt),
                    'Country': name_for_map
                })

        country_all_df_raw = pd.DataFrame(map_data).astype({
            'NumericCode': 'int64',
            'Count': 'int64'
        })
        
    st.session_state['debug_summary']['country_code_counts'] = country_code_counts
    st.session_state['debug_summary']['country_all_df'] = country_all_df_raw.to_dict('records')

    # --- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé »åº¦é›†è¨ˆ ---
    freq_map = st.session_state.get('target_freq_map', {})
    finished = st.session_state.get('finished_ips', set())
    freq_list = [{'Target_IP': t, 'Count': c} for t, c in freq_map.items() if t in finished]
    
    if freq_list:
        freq_full_df = pd.DataFrame(freq_list).sort_values('Count', ascending=False)
    else:
        freq_full_df = pd.DataFrame(columns=['Target_IP', 'Count'])
    
    if not freq_full_df.empty:
        freq_df = freq_full_df.head(10).copy()
    else:
        freq_df = pd.DataFrame(columns=['Target_IP', 'Count'])

    return isp_df, country_df, freq_df, country_all_df_raw, isp_full_df, country_full_df, freq_full_df

# --- é›†è¨ˆçµæœæç”»ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def draw_summary_content(isp_summary_df, country_summary_df, target_frequency_df, country_all_df, title):
    st.subheader(title)
    
    st.markdown("#### ğŸŒ å›½åˆ¥ IP ã‚«ã‚¦ãƒ³ãƒˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—")
    if WORLD_MAP_GEOJSON and not country_all_df.empty:
        
        base = alt.Chart(WORLD_MAP_GEOJSON).mark_geoshape(
            stroke='black', 
            strokeWidth=0.1
        ).encode(
            color=alt.value("#f0f0f052"), 
        ).project(
            type='mercator',
            scale=80,
            translate=[500, 180]        
        ).properties(
            title='World Map IP Count Heatmap',
            width=2500, 
            height=400 
        )

        heatmap = alt.Chart(WORLD_MAP_GEOJSON).mark_geoshape(
            stroke='black', 
            strokeWidth=0.1
        ).encode(
            color=alt.Color('Count:Q',
                            scale=alt.Scale(
                                type='log', 
                                domainMin=1,
                                domainMax=alt.Undefined,
                                range=['#99f6e4', '#facc15', '#dc2626']
                            ),
                            legend=alt.Legend(title="IP Count")),
            tooltip=[
                alt.Tooltip('Country:N', title='Country'),
                alt.Tooltip('Count:Q', title='IP Count', format=',')
            ]
        ).transform_lookup(
            lookup='id',
            from_=alt.LookupData(
                country_all_df, 
                key='NumericCode',          
                fields=['Count', 'Country']
            )
        ).project(
            type='mercator',
            scale=80,
            translate=[500, 180]
            )

        chart = alt.layer(base, heatmap).resolve_scale(
            color='independent'
        ).configure_legend(
            orient='right'
        ).interactive()
        
        st.altair_chart(chart, use_container_width=True)
        
    else:
        st.info("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯GeoJSONãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‹ã€æˆåŠŸã—ãŸIPv4ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
    
    st.markdown("---")


    col_freq, col_isp, col_country = st.columns([1, 1, 1]) 

    # å…±é€šãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆé–¢æ•°
    def create_labeled_bar_chart(df, x_field, y_field, title):
        base = alt.Chart(df).encode(
            x=alt.X(x_field, title='Count'),
            y=alt.Y(y_field, sort='-x', title=y_field),
            tooltip=[y_field, x_field]
        )
        bars = base.mark_bar()
        text = base.mark_text(
            align='left',
            baseline='middle',
            dx=3 
        ).encode(
            text=x_field
        )
        return (bars + text).properties(title=title).interactive()

    with col_freq:
        st.markdown("#### ğŸ¯ å¯¾è±¡IPåˆ¥ã‚«ã‚¦ãƒ³ãƒˆ (ãƒˆãƒƒãƒ—10)")
        if not target_frequency_df.empty:
            st.caption(f"**é›†è¨ˆå¯¾è±¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ•° (é‡è¤‡ãªã—):** {len(target_frequency_df)} ä»¶")
            chart = create_labeled_bar_chart(target_frequency_df, 'Count', 'Target_IP', 'Target IP Counts')
            st.altair_chart(chart, use_container_width=True)

            target_frequency_df_display = target_frequency_df.copy()
            target_frequency_df_display['Target_IP'] = target_frequency_df_display['Target_IP'].str.wrap(25)
            st.dataframe(target_frequency_df_display, hide_index=True, use_container_width=True)
        else:
            st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
    with col_isp:
        st.markdown("#### ğŸ¢ ISPåˆ¥ã‚«ã‚¦ãƒ³ãƒˆ (ãƒˆãƒƒãƒ—10)")
        if not isp_summary_df.empty:
            chart = create_labeled_bar_chart(isp_summary_df, 'Count', 'ISP', 'ISP Counts')
            st.altair_chart(chart, use_container_width=True)
            
            st.dataframe(isp_summary_df, hide_index=True, use_container_width=True)
        else:
            st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
    with col_country:
        st.markdown("#### ğŸŒ å›½åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ (ãƒˆãƒƒãƒ—10)")
        if not country_summary_df.empty:
            chart = create_labeled_bar_chart(country_summary_df, 'Count', 'Country', 'Country Counts')
            st.altair_chart(chart, use_container_width=True)
            
            st.dataframe(country_summary_df, hide_index=True, use_container_width=True)
        else:
            st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

# ğŸ’¡ HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–¢æ•°ï¼ˆæ”¹è‰¯ç‰ˆ v7.2ï¼‰
def generate_full_report_html(isp_full_df, country_full_df, freq_full_df):
    
    def create_chunked_chart_specs(df, x_col, y_col, title_base, chunk_size=50):
        specs = []
        # ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã§ã®æœ€å¤§å€¤ã‚’å–å¾— (ãƒšãƒ¼ã‚¸ã¾ãŸãã®ã‚¹ã‚±ãƒ¼ãƒ«çµ±ä¸€ã®ãŸã‚)
        global_max = df[x_col].max() if not df.empty else 0

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆ†å‰²
        chunks = [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]
        
        for i, chunk in enumerate(chunks):
            chart_title = f"{title_base} ({i+1}/{len(chunks)})" if len(chunks) > 1 else title_base
            
            # æ•°å€¤ãƒ©ãƒ™ãƒ«ä»˜ããƒãƒ£ãƒ¼ãƒˆ
            # ğŸ’¡ xè»¸ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å…¨ä½“æœ€å¤§å€¤ã§å›ºå®šã™ã‚‹
            base = alt.Chart(chunk).encode(
                x=alt.X(x_col, title='Count', scale=alt.Scale(domain=[0, global_max])),
                y=alt.Y(y_col, sort='-x', title=y_col),
                tooltip=[y_col, x_col]
            )
            bars = base.mark_bar()
            text = base.mark_text(
                align='left',
                baseline='middle',
                dx=5, 
                fontSize=11,
                fontWeight='bold'
            ).encode(
                text=x_col
            )
            chart = (bars + text).properties(
                title=chart_title,
                width=700,
                height=alt.Step(20) 
            )
            specs.append(chart.to_dict())
        return specs

    # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒãƒ£ãƒ¼ãƒˆã‚¹ãƒšãƒƒã‚¯ã‚’ç”Ÿæˆ
    target_specs = create_chunked_chart_specs(freq_full_df, 'Count', 'Target_IP', 'Target IP Counts (All)')
    isp_specs = create_chunked_chart_specs(isp_full_df, 'Count', 'ISP', 'ISP Counts (All)')
    country_specs = create_chunked_chart_specs(country_full_df, 'Count', 'Country', 'Country Counts (All)')

    # HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    html_template = f"""
    <!DOCTYPE html>
    <html>
    <head>
      <title>Whois Search Full Report</title>
      <script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
      <script src="https://cdn.jsdelivr.net/npm/vega-lite@5"></script>
      <script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
      <style>
        body {{ font-family: "Helvetica Neue", Arial, sans-serif; padding: 40px; background-color: #fff; color: #333; }}
        h1 {{ text-align: center; border-bottom: 2px solid #333; padding-bottom: 10px; margin-bottom: 30px; }}
        h2 {{ 
            color: #1e3a8a; 
            margin-top: 50px; 
            border-left: 5px solid #1e3a8a; 
            padding-left: 15px; 
            page-break-before: always; 
        }}
        h2:first-of-type {{ page-break-before: auto; }} 
        
        .chart-container {{ 
            margin-bottom: 40px; 
            padding: 10px; 
            page-break-inside: avoid; 
        }}
        
        @media print {{
            body {{ padding: 0; background-color: #fff; }}
            .no-print {{ display: none; }}
            h2 {{ margin-top: 20px; }}
        }}
      </style>
    </head>
    <body>
      <h1>Whoisæ¤œç´¢çµæœåˆ†æãƒ¬ãƒãƒ¼ãƒˆ</h1>
      <p style="text-align: center; color: #666;">Generated by Whois Search Tool</p>

      <h2>å¯¾è±¡IPã‚¢ãƒ‰ãƒ¬ã‚¹ ã‚«ã‚¦ãƒ³ãƒˆ (å…¨ {len(freq_full_df)} ä»¶)</h2>
      <div id="target_charts"></div>

      <h2>ISPåˆ¥ ã‚«ã‚¦ãƒ³ãƒˆ (å…¨ {len(isp_full_df)} ä»¶)</h2>
      <div id="isp_charts"></div>

      <h2>å›½åˆ¥ ã‚«ã‚¦ãƒ³ãƒˆ (å…¨ {len(country_full_df)} ä»¶)</h2>
      <div id="country_charts"></div>

      <script type="text/javascript">
        // Embed charts function
        function embedCharts(containerId, specs) {{
            const container = document.getElementById(containerId);
            specs.forEach((spec, index) => {{
                const div = document.createElement('div');
                div.id = containerId + '_' + index;
                div.className = 'chart-container';
                container.appendChild(div);
                vegaEmbed('#' + div.id, spec, {{actions: false}});
            }});
        }}

        // Data from Python (Serialized to JSON)
        const targetSpecs = {json.dumps(target_specs)};
        const ispSpecs = {json.dumps(isp_specs)};
        const countrySpecs = {json.dumps(country_specs)};

        // Render
        if (targetSpecs.length > 0) embedCharts('target_charts', targetSpecs);
        else document.getElementById('target_charts').innerHTML = '<p>ãƒ‡ãƒ¼ã‚¿ãªã—</p>';

        if (ispSpecs.length > 0) embedCharts('isp_charts', ispSpecs);
        else document.getElementById('isp_charts').innerHTML = '<p>ãƒ‡ãƒ¼ã‚¿ãªã—</p>';

        if (countrySpecs.length > 0) embedCharts('country_charts', countrySpecs);
        else document.getElementById('country_charts').innerHTML = '<p>ãƒ‡ãƒ¼ã‚¿ãªã—</p>';
      </script>
    </body>
    </html>
    """
    return html_template

# ğŸ“ˆ ã‚¯ãƒ­ã‚¹åˆ†æç”¨HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–¢æ•°
def generate_cross_analysis_html(chart_spec, x_col, group_col):
    html_template = f"""
    <!DOCTYPE html>
    <html>
    <head>
      <title>Whois Cross Analysis Report</title>
      <script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
      <script src="https://cdn.jsdelivr.net/npm/vega-lite@5"></script>
      <script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
      <style>
        body {{ font-family: "Helvetica Neue", Arial, sans-serif; padding: 40px; background-color: #fff; color: #333; }}
        h1 {{ text-align: center; border-bottom: 2px solid #333; padding-bottom: 10px; margin-bottom: 30px; }}
        .info {{ text-align: center; color: #666; margin-bottom: 20px; }}
        .chart-container {{ 
            width: 100%; 
            display: flex; 
            justify-content: center; 
            margin-bottom: 40px; 
            padding: 10px; 
        }}
      </style>
    </head>
    <body>
      <h1>ã‚¯ãƒ­ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {x_col} vs {group_col}</h1>
      <p class="info">Generated by Whois Search Tool</p>

      <div id="chart" class="chart-container"></div>

      <script type="text/javascript">
        const spec = {json.dumps(chart_spec)};
        vegaEmbed('#chart', spec, {{actions: true}});
      </script>
    </body>
    </html>
    """
    return html_template

# --- Excelç”Ÿæˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def convert_df_to_excel(df):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Sheet1')
    return output.getvalue()

# --- Advanced Excel Generator (Pivot & Chart) v5.0 ---
def create_advanced_excel(df, time_col_name=None):
    """
    1. Raw Data
    2. Report_ISP_Volume: ISP Ranking (Bar Chart)
    3. Report_ISP_Risk: ISP x ProxyType (Stacked Bar)
    4. Report_Time_Volume: Hour Trend (Bar/Line) [if time_col available]
    5. Report_Time_Risk: Hour x ProxyType (Stacked Bar) [if time_col available]
    """
    output = io.BytesIO()
    
    # 1. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    # Proxy Typeã®ç©ºæ¬„ã‚’ã€ŒStandard Connectionã€ã§åŸ‹ã‚ã‚‹ (ç”¨èªå¤‰æ›´)
    if 'Proxy Type' in df.columns:
        df['Proxy Type'] = df['Proxy Type'].fillna('Standard Connection')
        df['Proxy Type'] = df['Proxy Type'].replace('', 'Standard Connection')
        # å¤ã„ç”¨èªãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã®å¿µã®ç‚ºã®ç½®æ›
        df['Proxy Type'] = df['Proxy Type'].replace('Residential/Normal', 'Standard Connection')
        df['Proxy Type'] = df['Proxy Type'].replace('Residential/General', 'Standard Connection')
        df['Proxy Type'] = df['Proxy Type'].replace('Residential/Business', 'Standard Connection')
        df['Proxy Type'] = df['Proxy Type'].replace('nan', 'Standard Connection')
    else:
        df['Proxy Type'] = 'Standard Connection'
    
    # æ™‚é–“å¸¯åˆ—ã®ä½œæˆ
    has_time_analysis = False
    if time_col_name and time_col_name in df.columns:
        try:
            df['Hour'] = pd.to_datetime(df[time_col_name], errors='coerce').dt.hour
            has_time_analysis = True
        except Exception:
            pass

    # ã‚«ã‚¦ãƒ³ãƒˆç”¨ã®åˆ—ï¼ˆæœ€åˆã®åˆ—ã‚’ä½¿ã†ï¼‰
    count_col = df.columns[0]

    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        # Sheet 1: Raw Data
        df.to_excel(writer, index=False, sheet_name='Raw Data')
        wb = writer.book
        
        # --- å…±é€šãƒãƒ£ãƒ¼ãƒˆä½œæˆé–¢æ•° (è§£èª¬æ–‡ä»˜ã) ---
        def add_chart_sheet(pivot_df, sheet_name, chart_title, x_title, y_title, description, chart_type="col", stacked=False):
            if pivot_df.empty: return

            # Sheetä½œæˆã¨ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ (ãƒ˜ãƒƒãƒ€ãƒ¼ç”¨ã«å°‘ã—ä¸‹ã’ã‚‹)
            pivot_df.to_excel(writer, sheet_name=sheet_name, startrow=4)
            ws = wb[sheet_name]
            
            # --- è§£èª¬æ–‡ã®æŒ¿å…¥ ---
            ws['A1'] = chart_title
            ws['A1'].font = Font(size=14, bold=True, color="1E3A8A")
            
            ws['A2'] = description
            ws['A2'].font = Font(size=11, color="555555", italic=True)
            ws['A2'].alignment = Alignment(wrap_text=True, vertical="top")
            
            # ã‚»ãƒ«çµåˆ (èª¬æ˜æ–‡ã‚¨ãƒªã‚¢)
            ws.merge_cells('A2:H3')
            
            # å°åˆ·è¨­å®šï¼ˆæ¨ªå‘ãï¼‰
            ws.page_setup.orientation = ws.ORIENTATION_LANDSCAPE
            ws.page_setup.fitToWidth = 1
            ws.print_options.horizontalCentered = True
            
            # ã‚°ãƒ©ãƒ•ä½œæˆ
            chart = BarChart()
            chart.type = chart_type
            chart.style = 10
            chart.title = chart_title
            chart.y_axis.title = y_title
            chart.x_axis.title = x_title
            if stacked:
                chart.grouping = "stacked"
                chart.overlap = 100
            
            chart.height = 15 # å°åˆ·ç”¨ã«è¦‹ã‚„ã™ãå¤§ãã
            chart.width = 25

            # ãƒ‡ãƒ¼ã‚¿ç¯„å›²è¨­å®š (startrow=4 ãªã®ã§ãƒ‡ãƒ¼ã‚¿ã¯5è¡Œç›®ã‹ã‚‰)
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã¯ 5è¡Œç›®
            # ãƒ‡ãƒ¼ã‚¿é–‹å§‹ã¯ 6è¡Œç›®
            data_start_row = 5 
            data_end_row = data_start_row + len(pivot_df)
            
            data = Reference(ws, min_col=2, min_row=data_start_row, max_row=data_end_row, max_col=len(pivot_df.columns)+1)
            cats = Reference(ws, min_col=1, min_row=data_start_row+1, max_row=data_end_row)
            
            chart.add_data(data, titles_from_data=True)
            chart.set_categories(cats)
            
            # ã‚°ãƒ©ãƒ•é…ç½® (ãƒ‡ãƒ¼ã‚¿ã®ä¸‹ã§ã¯ãªãæ¨ªã«é…ç½®ã—ã¦è¦‹ã‚„ã™ã)
            ws.add_chart(chart, "E5")

        # ---------------------------------------------------------
        # 2. Report_ISP_Volume: [ISP_JP] x [Count]
        # ---------------------------------------------------------
        top_isps = df['ISP_JP'].value_counts().head(20).index
        df_isp = df[df['ISP_JP'].isin(top_isps)]
        pivot_isp_vol = df_isp.pivot_table(
            index='ISP_JP', 
            values=count_col, 
            aggfunc='count'
        ).sort_values(count_col, ascending=False)
        
        desc_isp_vol = "ã©ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæœ€ã‚‚å¤šã„ã‹ã‚’å¯è¦–åŒ–ã—ã¦ã„ã¾ã™ã€‚ç‰¹å®šã®ISPã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹é›†ä¸­ã¯ã€ãã®ã‚µãƒ¼ãƒ“ã‚¹ã®åˆ©ç”¨è€…å±¤ã¾ãŸã¯ç‰¹å®šã®ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã®å½±éŸ¿ã‚’ç¤ºå”†ã—ã¾ã™ã€‚"
        add_chart_sheet(pivot_isp_vol, 'Report_ISP_Volume', 'ISP Access Volume Ranking (Top 20)', 'Internet Service Provider', 'Access Count (ä»¶æ•°)', desc_isp_vol)

        # ---------------------------------------------------------
        # 3. Report_ISP_Risk: [ISP_JP] x [Proxy Type]
        # ---------------------------------------------------------
        pivot_isp_risk = df_isp.pivot_table(
            index='ISP_JP', 
            columns='Proxy Type', 
            values=count_col, 
            aggfunc='count', 
            fill_value=0
        )
        desc_isp_risk = "ãã®ISPãŒå®‰å…¨ãªä¸€èˆ¬å›ç·šã‹ã€æ³¨æ„ãŒå¿…è¦ãªã‚µãƒ¼ãƒãƒ¼/VPNçµŒç”±ã‹ã‚’åˆ¤å®šã—ã¦ã„ã¾ã™ã€‚ã€ŒStandard Connectionã€ã¯ä¸€èˆ¬çš„ãªå®‰å…¨ãªæ¥ç¶šã§ã™ã€‚ã€ŒHostingã€ã‚„ã€ŒVPNã€ãŒå¤šã„å ´åˆã¯æ©Ÿæ¢°çš„ãªã‚¢ã‚¯ã‚»ã‚¹ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        add_chart_sheet(pivot_isp_risk, 'Report_ISP_Risk', 'Risk Analysis by ISP (Top 20)', 'Internet Service Provider', 'Access Count (ä»¶æ•°)', desc_isp_risk, stacked=True)
        
        # ---------------------------------------------------------
        # 4. Report_Country: [Country_JP] x [Count] (Bonus)
        # ---------------------------------------------------------
        pivot_country = df.pivot_table(
            index='Country_JP',
            values=count_col,
            aggfunc='count'
        ).sort_values(count_col, ascending=False).head(15)
        desc_country = "å›½ã”ã¨ã®ã‚¢ã‚¯ã‚»ã‚¹æ•°ã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°åŒ–ã—ã¦ã„ã¾ã™ã€‚ã‚µãƒ¼ãƒ“ã‚¹æä¾›ã‚¨ãƒªã‚¢å¤–ã‹ã‚‰ã®äºˆæœŸã›ã¬ã‚¢ã‚¯ã‚»ã‚¹æ¤œçŸ¥ã‚„ã€æµ·å¤–ã‹ã‚‰ã®æ”»æ’ƒäºˆå…†ã®ç™ºè¦‹ã«å½¹ç«‹ã¡ã¾ã™ã€‚"
        add_chart_sheet(pivot_country, 'Report_Country', 'Country Access Volume (Top 15)', 'Country Name', 'Access Count (ä»¶æ•°)', desc_country)

        # ---------------------------------------------------------
        # 5. Time Analysis (if available)
        # ---------------------------------------------------------
        if has_time_analysis:
            # Report_Time_Volume: [Hour] x [Count]
            pivot_time_vol = df.pivot_table(
                index='Hour',
                values=count_col,
                aggfunc='count',
                fill_value=0
            ).reindex(range(24), fill_value=0)
            desc_time_vol = "ä½•æ™‚ã«ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­ã—ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã—ã¦ã„ã¾ã™ã€‚ä¸€èˆ¬çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æ´»å‹•æ™‚é–“å¸¯ã«ã€Botãªã©ã¯æ·±å¤œæ—©æœã‚„24æ™‚é–“ä¸€å®šã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¡Œã†å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚"
            add_chart_sheet(pivot_time_vol, 'Report_Time_Volume', 'Hourly Access Trend', 'Time of Day (0-23h)', 'Access Count (ä»¶æ•°)', desc_time_vol)

            # Report_Time_Risk: [Hour] x [Proxy Type]
            pivot_time_risk = df.pivot_table(
                index='Hour',
                columns='Proxy Type',
                values=count_col,
                aggfunc='count',
                fill_value=0
            ).reindex(range(24), fill_value=0)
            desc_time_risk = "æ·±å¤œå¸¯ãªã©ã«æ€ªã—ã„ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆHosting/VPNç­‰ï¼‰ãŒå¢—ãˆã¦ã„ãªã„ã‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚å¤œé–“ã«Hostingåˆ¤å®šãŒå¢—åŠ ã™ã‚‹å ´åˆã€Botã«ã‚ˆã‚‹è‡ªå‹•å·¡å›ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            add_chart_sheet(pivot_time_risk, 'Report_Time_Risk', 'Hourly Risk Trend', 'Time of Day (0-23h)', 'Access Count (ä»¶æ•°)', desc_time_risk, stacked=True)
            
    return output.getvalue()


def display_results(results, current_mode_full_text, display_mode):
    st.markdown("### ğŸ“ æ¤œç´¢çµæœ")

    with st.expander("âš ï¸ åˆ¤å®šã‚¢ã‚¤ã‚³ãƒ³ã¨è¡¨ç¤ºãƒ«ãƒ¼ãƒ«ã«ã¤ã„ã¦"):
        st.info("""
        ### ğŸ” åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®æ¦‚è¦
        æœ¬ãƒ„ãƒ¼ãƒ«ã¯ã€IPã‚¢ãƒ‰ãƒ¬ã‚¹ã«ç´ä»˜ã‘ã‚‰ã‚ŒãŸ**ASNï¼ˆAutonomous System Numberï¼‰ãŠã‚ˆã³ISPï¼ˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‚µãƒ¼ãƒ“ã‚¹ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼‰ã®åç§°ãƒ»å±æ€§**ã‚’è§£æã—ã€é€šä¿¡ä¸»ä½“ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¨®åˆ¥ã‚’è‡ªå‹•çš„ã«åˆ†é¡ã—ã¦ã„ã¾ã™ã€‚
        
        ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã®é€šä¿¡ã¯ã€ãã®ç”¨é€”ã«å¿œã˜ã¦ã€Œå€‹äººå®…ãƒ»æ³•äººæ‹ ç‚¹ã‹ã‚‰ã®ç›´æ¥æ¥ç¶šã€ã¨ã€Œéå¯¾é¢çš„ãªä¸­ç¶™ãƒ»ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°çµŒç”±ã®æ¥ç¶šã€ã«å¤§åˆ¥ã•ã‚Œã¾ã™ã€‚æœ¬æ©Ÿèƒ½ã¯å¾Œè€…ã‚’æ¤œçŸ¥ã—ã€èª¿æŸ»ã®å„ªå…ˆé †ä½åˆ¤æ–­ã‚’æ”¯æ´ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚
        
        ---
        
        ### ğŸ“Œ åˆ¤å®šç¨®åˆ¥ã®å®šç¾©ã¨æŠ€è¡“çš„èƒŒæ™¯
        
        - **âš ï¸ [Tor Node]**
            - **å®šç¾©**: Torï¼ˆThe Onion Routerï¼‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹ã€ŒExit Nodeï¼ˆå‡ºå£ãƒãƒ¼ãƒ‰ï¼‰ã€ã‚’æŒ‡ã—ã¾ã™ã€‚
            - **èƒŒæ™¯**: èµ·å‹•æ™‚ã«Tor Projectå…¬å¼ã‚µã‚¤ãƒˆã‚ˆã‚Šæœ€æ–°ã®ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã€ç…§åˆã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚é«˜ã„åŒ¿åæ€§ã‚’ç¶­æŒã—ãŸé€šä¿¡ã§ã‚ã‚‹ãŸã‚ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ã®æ¤œè¨ãŒå¿…è¦ã§ã™ã€‚
            
        - **âš ï¸ [VPN/Proxy]**
            - **å®šç¾©**: å•†ç”¨VPNã‚µãƒ¼ãƒ“ã‚¹ã€å…¬é–‹ãƒ—ãƒ­ã‚­ã‚·ã€ã¾ãŸã¯ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã‚’ç›®çš„ã¨ã—ãŸä¸­ç¶™å›£ä½“ã«å±ã™ã‚‹IPã§ã™ã€‚
            - **èƒŒæ™¯**: ISPåç§°ã«å«ã¾ã‚Œã‚‹ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆVPN, Proxyç­‰ï¼‰ãŠã‚ˆã³æ—¢çŸ¥ã®åŒ¿ååŒ–ã‚µãƒ¼ãƒ“ã‚¹é‹å–¶çµ„ç¹”åã«åŸºã¥ãåˆ¤åˆ¥ã—ã¾ã™ã€‚
            
        - **âš ï¸ [Hosting/Infra]**
            - **å®šç¾©**: ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆAWS, Azure, GCPç­‰ï¼‰ã‚„ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ã€ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°äº‹æ¥­è€…ã®ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã§ã™ã€‚
            - **èƒŒæ™¯**: ä¸€èˆ¬çš„ãªã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒå›ç·šã¨ã¯ç•°ãªã‚Šã€ã‚µãƒ¼ãƒãƒ¼é–“é€šä¿¡ã‚„Botã€ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã€ã‚ã‚‹ã„ã¯æ”»æ’ƒç”¨ã‚¤ãƒ³ãƒ•ãƒ©ã¨ã—ã¦åˆ©ç”¨ã•ã‚Œã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„ãƒãƒ¼ãƒ‰ã§ã™ã€‚
            
        ---
        
        â€» æœ¬åˆ¤å®šã¯ISPåç§°ç­‰ã«åŸºã¥ãæ¨è«–ã§ã‚ã‚‹ãŸã‚ã€å®Ÿéš›ã®åˆ©ç”¨çŠ¶æ³ã¨ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
        """)
    
    col_widths = [0.5, 1.5, 1.2, 2.0, 1.5, 1.5, 1.0, 1.2, 0.5] 
    h_cols = st.columns(col_widths)
    headers = ["No.", "Target IP", "å›½å","ISP(æ—¥æœ¬èª)", "RIR Link", "Security Links", "Proxy Type",  "Status", "âœ…"]
    for col, name in zip(h_cols, headers):
        col.markdown(f"**{name}**")
    st.markdown("<hr style='margin: 0px 0px 10px 0px;'>", unsafe_allow_html=True)

    with st.container(height=800):
        if not results:
            st.info("æ¤œç´¢çµæœãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
            return

        for idx, res in enumerate(results):
                row_cols = st.columns(col_widths)
                row_cols[0].write(f"**{idx+1}**")
                
                target_ip = res.get('Target_IP', 'N/A')
                row_cols[1].markdown(f"`{target_ip}`")
                
                c_jp = res.get('Country_JP', 'N/A')
                c_en = res.get('Country', 'N/A')
                row_cols[2].write(f"{c_jp}\n({c_en})")
                
                isp_display = res.get('ISP_JP', res.get('ISP', 'N/A'))
                row_cols[3].write(isp_display)
                
                rir_link = res.get('RIR_Link', 'N/A')
                with row_cols[4]:
                    st.write(rir_link)
                    clean_ip = get_copy_target(target_ip)
                    st.code(clean_ip, language=None)
                
                row_cols[5].write(res.get('Secondary_Security_Links', 'N/A'))
                hosting_val = res.get('Proxy_Type', '')
                row_cols[6].write(hosting_val)          
                
                status_val = res.get('Status', 'N/A')
                if "Success" in status_val:
                    row_cols[7].markdown(f"<span style='color:green;'>{status_val}</span>", unsafe_allow_html=True)
                else:
                    row_cols[7].write(status_val)
                    
                row_cols[8].checkbox("é¸æŠ", key=f"chk_{get_copy_target(target_ip)}_{idx}", label_visibility="collapsed")


# ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿çµåˆåˆ†ææ©Ÿèƒ½
def render_merged_analysis(df_merged):
    st.markdown("### ğŸ“ˆ å…ƒãƒ‡ãƒ¼ã‚¿ x æ¤œç´¢çµæœ ã‚¯ãƒ­ã‚¹åˆ†æ")
    st.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å…ƒã®åˆ—ã¨ã€æ¤œç´¢ã§å¾—ã‚‰ã‚ŒãŸWhoisæƒ…å ±ã‚’çµ„ã¿åˆã‚ã›ã¦å¯è¦–åŒ–ã—ã¾ã™ã€‚å°åˆ·ç”¨ã«ã‚°ãƒ©ãƒ•å˜ä½“ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚‚å¯èƒ½ã§ã™ã€‚")
    
    # ã‚°ãƒ©ãƒ•è¨­å®šç”¨ã‚«ãƒ©ãƒ 
    # å…ƒãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ï¼ˆStatusãªã©å¾Œä»˜ã‘ã®ã‚«ãƒ©ãƒ ã‚’é™¤ãï¼‰
    original_cols = [c for c in df_merged.columns if c not in ['ISP', 'ISP_JP', 'Country', 'Country_JP', 'Proxy Type', 'Status']]
    # Whoisçµæœã®ã‚«ãƒ©ãƒ 
    whois_cols = ['Country_JP', 'ISP_JP', 'Proxy Type', 'Status']
    
    col_x, col_grp, col_chart_type = st.columns(3)
    
    with col_x:
        x_col = st.selectbox("Xè»¸ (ã‚«ãƒ†ã‚´ãƒª/å…ƒã®åˆ—)", original_cols + whois_cols, index=0)
    
    with col_grp:
        group_col = st.selectbox("ç©ã¿ä¸Šã’/è‰²åˆ†ã‘ (Whoisæƒ…å ±ãªã©)", ['(ãªã—)'] + whois_cols + original_cols, index=1)
        
    with col_chart_type:
        chart_type = st.radio("ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒ—", ["ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆ (é›†è¨ˆ)", "ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—"], horizontal=True)

    if not df_merged.empty:
        chart = None
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†: NaNã‚’æ–‡å­—åˆ—ã«ç½®æ›ã—ã¦Altairã®ã‚¨ãƒ©ãƒ¼å›é¿
        chart_df = df_merged.fillna("N/A").astype(str)

        if chart_type == "ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆ (é›†è¨ˆ)":
            if group_col != '(ãªã—)':
                chart = alt.Chart(chart_df).mark_bar().encode(
                    x=alt.X(x_col, title=x_col),
                    y=alt.Y('count()', title='ä»¶æ•°'),
                    color=alt.Color(group_col, title=group_col),
                    tooltip=[x_col, group_col, 'count()']
                ).properties(height=400)
            else:
                chart = alt.Chart(chart_df).mark_bar().encode(
                    x=alt.X(x_col, title=x_col),
                    y=alt.Y('count()', title='ä»¶æ•°'),
                    tooltip=[x_col, 'count()']
                ).properties(height=400)
                
        elif chart_type == "ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—":
             if group_col != '(ãªã—)':
                chart = alt.Chart(chart_df).mark_rect().encode(
                    x=alt.X(x_col, title=x_col),
                    y=alt.Y(group_col, title=group_col),
                    color=alt.Color('count()', title='ä»¶æ•°', scale=alt.Scale(scheme='viridis')),
                    tooltip=[x_col, group_col, 'count()']
                ).properties(height=400)
             else:
                 st.warning("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã«ã¯ã€Œç©ã¿ä¸Šã’/è‰²åˆ†ã‘ã€é …ç›®ã®é¸æŠãŒå¿…è¦ã§ã™ã€‚")

        if chart:
            st.altair_chart(chart, use_container_width=True)
            
            # HTMLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨
            chart_json = chart.to_dict()
            html_content = generate_cross_analysis_html(chart_json, x_col, group_col if group_col != '(ãªã—)' else 'Count')
            
            st.download_button(
                label="â¬‡ï¸ ã‚¯ãƒ­ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ(HTML)ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=html_content,
                file_name=f"cross_analysis_{x_col}_vs_{group_col}.html",
                mime="text/html",
                help="ã‚°ãƒ©ãƒ•ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§å…¨ç”»é¢è¡¨ç¤ºã—ã€å°åˆ·ã™ã‚‹ã®ã«é©ã—ã¦ã„ã¾ã™ã€‚"
            )


# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    if 'cancel_search' not in st.session_state: st.session_state['cancel_search'] = False
    if 'raw_results' not in st.session_state: st.session_state['raw_results'] = []
    if 'targets_cache' not in st.session_state: st.session_state['targets_cache'] = []
    if 'is_searching' not in st.session_state: st.session_state['is_searching'] = False
    if 'deferred_ips' not in st.session_state: st.session_state['deferred_ips'] = {} 
    if 'finished_ips' not in st.session_state: st.session_state['finished_ips'] = set() 
    if 'search_start_time' not in st.session_state: st.session_state['search_start_time'] = 0.0 
    if 'target_freq_map' not in st.session_state: st.session_state['target_freq_map'] = {} 
    if 'cidr_cache' not in st.session_state: st.session_state['cidr_cache'] = {} 
    if 'debug_summary' not in st.session_state: st.session_state['debug_summary'] = {}

    tor_nodes = fetch_tor_exit_nodes()
    
    with st.sidebar:
        st.markdown("### ğŸ› ï¸ Menu")
        selected_menu = option_menu(
            menu_title=None,
            options=["Whoisæ¤œç´¢", "ä»•æ§˜ãƒ»è§£èª¬"],
            icons=["search", "book"],
            default_index=0,
            styles={
                "nav-link": {"font-size": "16px", "text-align": "left", "margin": "5px", "--hover-color": "#eee"},
                "nav-link-selected": {"background-color": "#1e3a8a"},
            }
        )
        st.markdown("---")
        if st.button("ğŸ”„ IPã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢", help="ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¤ããªã£ãŸå ´åˆã«ã‚¯ãƒªãƒƒã‚¯"):
            st.session_state['cidr_cache'] = {} 
            st.info("IP/CIDRã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
            st.rerun()

    if selected_menu == "ä»•æ§˜ãƒ»è§£èª¬":
        st.title("ğŸ“– ãƒ„ãƒ¼ãƒ«ã®ä»•æ§˜ã¨è§£èª¬")
        st.markdown(f"""
        ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€IPã‚¢ãƒ‰ãƒ¬ã‚¹ã¾ãŸã¯ãƒ‰ãƒ¡ã‚¤ãƒ³åã«å¯¾ã—ã¦ Whois ãŠã‚ˆã³ IP Geolocation æƒ…å ±ã‚’ä¸€æ‹¬ã§æ¤œç´¢ã™ã‚‹ãŸã‚ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚
                    
        #### 1. ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
        - **IP Geolocation / ISP æƒ…å ±**: `ip-api.com` ã® API ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
        - **Whois ãƒªãƒ³ã‚¯**: å„IPã‚¢ãƒ‰ãƒ¬ã‚¹ã®å›½ã‚³ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ã€é©åˆ‡ãªåœ°åŸŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ¬ã‚¸ã‚¹ãƒˆãƒª (RIR) ã® Whois æ¤œç´¢ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆã—ã¾ã™ã€‚          

        #### 2. ä¸»ãªæ©Ÿèƒ½
        - **ä¸€æ‹¬Whoisæ¤œç´¢**: 
            - è¤‡æ•°è¡Œã® IP/ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ä¸€æ‹¬ã§å‡¦ç†ã§ãã¾ã™ã€‚
            - å‡¦ç†ã¯ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§è¡Œã‚ã‚Œã€APIã®ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã‚’è‡ªå‹•ã§æ¤œçŸ¥ãƒ»å¾…æ©Ÿã—ã¾ã™ã€‚
            - **CIDRã‚­ãƒ£ãƒƒã‚·ãƒ¥**: **åŒã˜CIDRãƒ–ãƒ­ãƒƒã‚¯å†…ã®IPã‚¢ãƒ‰ãƒ¬ã‚¹ã¯APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—**ã—ã€æ¤œç´¢é€Ÿåº¦ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã™ã€‚ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§IPv4ã¯/24ã€IPv6ã¯/48ã®ãƒ–ãƒ­ãƒƒã‚¯ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚ï¼‰
            - **æ¨™æº–ãƒ¢ãƒ¼ãƒ‰**: å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’å€‹åˆ¥ã«è¡¨ç¤ºã—ã¾ã™ã€‚
            - **é›†ç´„ãƒ¢ãƒ¼ãƒ‰**: åŒã˜ ISP/å›½ã‚³ãƒ¼ãƒ‰ã‚’æŒã¤é€£ç¶šã™ã‚‹IPv4ã‚¢ãƒ‰ãƒ¬ã‚¹ç¾¤ã‚’ã€ŒIPãƒ¬ãƒ³ã‚¸ã€ã¨ã—ã¦é›†ç´„è¡¨ç¤ºã—ã¾ã™ã€‚
            - **ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰**: APIã‚³ãƒ¼ãƒ«ã‚’è¡Œã‚ãšã€å„ç¨®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£/Whoisæ¤œç´¢ã‚µã‚¤ãƒˆã¸ã®ãƒªãƒ³ã‚¯ã®ã¿ã‚’æä¾›ã—ã¾ã™ã€‚
        - **é›†è¨ˆçµæœ**: æ¤œç´¢å¾Œã€ISPåˆ¥ã€å›½åˆ¥ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥ã‚’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤ºã—ã€å›½åˆ¥ã®IPã‚«ã‚¦ãƒ³ãƒˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚‚è¡¨ç¤ºã—ã¾ã™ã€‚
            - **ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œ**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã«ã‚ˆã‚ŠAPIãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIPã«é™å®šã•ã‚Œã¾ã™ãŒã€**é›†è¨ˆæ©Ÿèƒ½ã¯å…¥åŠ›ãƒªã‚¹ãƒˆã®IPã®é‡è¤‡åº¦ï¼ˆå‡ºç¾å›æ•°ï¼‰ã‚’æ­£ç¢ºã«åæ˜ **ã—ã¦ã„ã¾ã™ã€‚
        - **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£/Whoisæ¤œç´¢ã‚µã‚¤ãƒˆãƒªãƒ³ã‚¯**: VirusTotal, Aguseãªã©ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£ãƒªãƒ³ã‚¯ã‚‚ä½µã›ã¦è¡¨ç¤ºã—ã¾ã™ã€‚
                    
        #### 3. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£/Whoisæ¤œç´¢ã‚µã‚¤ãƒˆã®ç‰¹æ€§
        - **å…¬å¼RIR**: å„åœ°åŸŸã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ¬ã‚¸ã‚¹ãƒˆãƒª (RIR) ãŒæä¾›ã™ã‚‹å…¬å¼ã® Whois ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚æœ€ã‚‚æ­£ç¢ºãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œã¾ã™ãŒã€ä¸€éƒ¨ã® RIR ã§ã¯æ‰‹å‹•ã§ã®æ¤œç´¢ãŒå¿…è¦ã§ã™ã€‚
        - **[{'VirusTotal'}]({SECONDARY_TOOL_BASE_LINKS['VirusTotal']})**: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è©•åˆ¤ã€ãƒãƒ«ã‚¦ã‚§ã‚¢ã€æ”»æ’ƒå±¥æ­´ã®ç¢ºèªã§ãã¾ã™ã€‚
        - **[{'Whois.com'}]({SECONDARY_TOOL_BASE_LINKS['Whois.com']}) / [{'Who.is'}]({SECONDARY_TOOL_BASE_LINKS['Who.is']})**: å…¬å¼æƒ…å ±ã‚’è¦‹ã‚„ã™ãè¡¨ç¤ºã—ã¾ã™ã€‚ãƒ‰ãƒ¡ã‚¤ãƒ³/IPã®ä¸¡æ–¹ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚            
        - **[{'DomainSearch.jp'}]({SECONDARY_TOOL_BASE_LINKS['DomainSearch.jp']}) / [{'Aguse'}]({SECONDARY_TOOL_BASE_LINKS['Aguse']})**: IPã‚¢ãƒ‰ãƒ¬ã‚¹ã€ãƒ‰ãƒ¡ã‚¤ãƒ³åã€ãƒãƒ¼ãƒ ã‚µãƒ¼ãƒç­‰ã®è¤‡åˆçš„ãªèª¿æŸ»ãŒå¯èƒ½ã§ã™ã€‚
        - **[{'IP2Proxy'}]({SECONDARY_TOOL_BASE_LINKS['IP2Proxy']})**: ãƒ—ãƒ­ã‚­ã‚·ã€VPNã€Torãªã©ã®åŒ¿ååŒ–æŠ€è¡“ã®ä½¿ç”¨åˆ¤å®šãŒå¯èƒ½ã§ã™ã€‚
        - **[{'DNS Checker'}]({SECONDARY_TOOL_BASE_LINKS['DNS Checker']})**: IPv6å¯¾å¿œã€‚DNSãƒ¬ã‚³ãƒ¼ãƒ‰ã‚„Whoisæƒ…å ±ã®å¤šæ©Ÿèƒ½ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        - **[{'DNSlytics'}]({SECONDARY_TOOL_BASE_LINKS['DNSlytics']}) / [{'IP Location'}]({SECONDARY_TOOL_BASE_LINKS['IP Location']})**: IPv6å¯¾å¿œã€‚åœ°ç†æƒ…å ±ã€ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°æƒ…å ±ç­‰ã®èª¿æŸ»ãŒå¯èƒ½ã§ã™ã€‚
        - **[{'CP-WHOIS'}]({SECONDARY_TOOL_BASE_LINKS['CP-WHOIS']})**: **ä¿¡é ¼æ€§**ãŒé«˜ã„Whoisæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚åˆ©ç”¨è€…èªè¨¼ãŒå¿…è¦ã§ã™ã€‚

        #### 4. æŠ€è¡“çš„ä»•æ§˜
        - **Streamlit**: WebUIãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
        - **Requests/ThreadPoolExecutor**: HTTPé€šä¿¡ã¨ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—å‡¦ç†
        - **IP Address/Socket/Struct**: IPã‚¢ãƒ‰ãƒ¬ã‚¹æ“ä½œãŠã‚ˆã³CIDRå¯¾å¿œ
        - **Pandas/Altair/GeoJSON**: ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã¨å¯è¦–åŒ–
        - **Excelå‡ºåŠ›**: `openpyxl` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€é›†è¨ˆçµæœã‚„æ¤œç´¢ãƒªã‚¹ãƒˆã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚
        - **Tor Exit Node åˆ¤å®š**:
            - èµ·å‹•æ™‚ã«Torå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ã®å‡ºå£ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’è‡ªå‹•å–å¾—ãƒ»æ›´æ–°ã—ã€åŒ¿ååŒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµŒç”±ã®é€šä¿¡ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã¾ã™ã€‚

        #### 5. API ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–
        `ip-api.com` ã® API ã¯ç„¡æ–™ç‰ˆã§**æ¯åˆ† 45ãƒªã‚¯ã‚¨ã‚¹ãƒˆ**ã®ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãŒã‚ã‚Šã¾ã™ã€‚
        - **API å‡¦ç†ãƒ¢ãƒ¼ãƒ‰**ã§ã€å®‰å®šæ€§ã‚’å„ªå…ˆã™ã‚‹ã‹ã€é€Ÿåº¦ã‚’å„ªå…ˆã™ã‚‹ã‹ã‚’é¸æŠã§ãã¾ã™ã€‚
            - **å®‰å®šæ€§é‡è¦–**: å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã€APIã‚³ãƒ¼ãƒ«é–“ã« {MODE_SETTINGS["å®‰å®šæ€§é‡è¦– (2.5ç§’å¾…æ©Ÿ/å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰)"]["DELAY_BETWEEN_REQUESTS"]} ç§’ã®é…å»¶ã‚’è¨­ã‘ã¾ã™ã€‚
            - **é€Ÿåº¦å„ªå…ˆ**: 2ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã€APIã‚³ãƒ¼ãƒ«é–“ã« {MODE_SETTINGS["é€Ÿåº¦å„ªå…ˆ (1.4ç§’å¾…æ©Ÿ/2ã‚¹ãƒ¬ãƒƒãƒ‰)"]["DELAY_BETWEEN_REQUESTS"]} ç§’ã®é…å»¶ã‚’è¨­ã‘ã¾ã™ã€‚
        - æ¤œç´¢å‡¦ç†ä¸­ã« 429 ã‚¨ãƒ©ãƒ¼ (Too Many Requests) ãŒç™ºç”Ÿã—ãŸå ´åˆã€ãƒ„ãƒ¼ãƒ«ã¯è‡ªå‹•çš„ã« {RATE_LIMIT_WAIT_SECONDS} ç§’é–“å‡¦ç†ã‚’ä¸­æ–­ã—ã€ãã®å¾Œæ®‹ã‚Šã®å‡¦ç†ã‚’å†é–‹ã—ã¾ã™ã€‚
        - **CIDRã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½**ã«ã‚ˆã‚Šã€ä¸€åº¦æ¤œç´¢ã—ãŸIPã‚¢ãƒ‰ãƒ¬ã‚¹ã¨åŒã˜CIDRãƒ–ãƒ­ãƒƒã‚¯å†…ã®IPã‚¢ãƒ‰ãƒ¬ã‚¹ã«å¯¾ã™ã‚‹APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å›é¿ã—ã€ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–ã®åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã¦ã„ã¾ã™ã€‚
        
        #### 6. OCRã‚¨ãƒ©ãƒ¼å¯¾ç­–
        å…¥åŠ›ã•ã‚ŒãŸæ–‡å­—åˆ—ã«å¯¾ã—ã¦ã€OCRèª¤èªè­˜ã§ç™ºç”Ÿã—ã‚„ã™ã„æ–‡å­— (`â…¡` -> `11`,`I/l` -> `1`, `O/o` -> `0`, `S/s` -> `5` ãªã©) ã‚’è‡ªå‹•ã§ä¿®æ­£ã™ã‚‹å‡¦ç†ã‚’åŠ ãˆã¦ã„ã¾ã™ã€‚

    
        #### 7. åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã¨é€šä¿¡ã®ä»•çµ„ã¿
        - **åŒ¿ååŒ–ãƒ»ã‚¤ãƒ³ãƒ•ãƒ©åˆ¤å®š (Hosting/VPN/Proxy)**:
            - ISPåã‚„çµ„ç¹”åã« `hosting`, `cloud`, `vps`, `prox`, `vpn` ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã€**ã€Œâš ï¸ Hosting/VPN/Proxyã€**ã¨ã—ã¦è­¦å‘Šã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
            - ã“ã‚Œã¯ã€ãã®é€šä¿¡ãŒä¸€èˆ¬å®¶åº­ã®PCã‹ã‚‰ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ä¸Šã®ã‚µãƒ¼ãƒãƒ¼ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼‰ã‚„ä¸­ç¶™ã‚µãƒ¼ãƒãƒ¼ã‚’çµŒç”±ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
        - **CDNã‚„ä¸­ç¶™ã‚µãƒ¼ãƒ“ã‚¹ã®ç‰¹æ€§**:
            - **Cloudflare / Akamai / Google**: ã“ã‚Œã‚‰ã¯ä¸–ç•Œçš„ãªä¸­ç¶™æ‹ ç‚¹ï¼ˆCDNï¼‰ã‚„ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¤ãƒ³ãƒ•ãƒ©ã§ã™ã€‚ã“ã‚Œã‚‰ãŒã‚¢ã‚¯ã‚»ã‚¹å…ƒã¨ã—ã¦è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹å ´åˆã€å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·æ©Ÿèƒ½ï¼ˆiCloudãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒªãƒ¬ãƒ¼ç­‰ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ã€ã‚ã‚‹ã„ã¯ãƒœãƒƒãƒˆã«ã‚ˆã‚‹è‡ªå‹•å·¡å›ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
        """) 
        return
            

    # --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼šWhoisæ¤œç´¢ã‚¿ãƒ– ---
    st.title("ğŸŒ WhoisSearchTool")

    col_input1, col_input2 = st.columns([1, 1])

    with col_input1:
        manual_input = st.text_area(
            "ğŸ“‹ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ› (IP/ãƒ‰ãƒ¡ã‚¤ãƒ³)",
            height=150,
            placeholder="8.8.8.8\nexample.com\n2404:6800:..."
        )

    with col_input2:
        # --- ãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ¶é™ã®åˆ‡ã‚Šæ›¿ãˆ ---
        if IS_PUBLIC_MODE:
            # å…¬é–‹ãƒ¢ãƒ¼ãƒ‰ (stç‰ˆã®æŒ™å‹•): txtã®ã¿è¨±å¯ã€è­¦å‘Šã‚ã‚Š
            allowed_types = ['txt']
            label_text = "ğŸ“‚ IPãƒªã‚¹ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (.txtã®ã¿)"
            help_text = "â€» 1è¡Œã«1ã¤ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’è¨˜è¼‰"
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ (myç‰ˆã®æŒ™å‹•): csv/excelè¨±å¯
            allowed_types = ['txt', 'csv', 'xlsx', 'xls']
            label_text = "ğŸ“‚ ãƒªã‚¹ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (txt/csv/xlsx)"
            help_text = "â€» 1è¡Œã«1ã¤ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’è¨˜è¼‰ã€ã¾ãŸã¯CSV/Excelã®IPåˆ—ã‚’è‡ªå‹•æ¤œå‡ºã—ã¾ã™"

        uploaded_file = st.file_uploader(label_text, type=allowed_types)
        st.caption(help_text)
        
        raw_targets = []
        df_orig = None # åˆæœŸåŒ–

        if manual_input:
            raw_targets.extend(manual_input.splitlines())
        
        if uploaded_file:
            # --- å…¬é–‹ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®èª­ã¿è¾¼ã¿å‡¦ç† (stç‰ˆãƒ­ã‚¸ãƒƒã‚¯) ---
            if IS_PUBLIC_MODE:
                 try:
                    # ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èª­ã¿è¾¼ã‚€
                    string_data = uploaded_file.read().decode("utf-8")
                    raw_targets.extend(string_data.splitlines())
                    
                    # å…ƒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–
                    st.session_state['original_df'] = None
                    st.session_state['ip_column_name'] = None
                    
                    st.info(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(raw_targets)} è¡Œ")

                 except Exception as e:
                    st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            
            # --- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®èª­ã¿è¾¼ã¿å‡¦ç† (myç‰ˆãƒ­ã‚¸ãƒƒã‚¯) ---
            else:
                ip_col = None
                try:
                    if uploaded_file.name.endswith('.csv'):
                        df_orig = pd.read_csv(uploaded_file)
                    elif uploaded_file.name.endswith(('.xlsx', '.xls')):
                        df_orig = pd.read_excel(uploaded_file)
                    else:
                        # TXTãƒ•ã‚¡ã‚¤ãƒ«
                        raw_targets.extend(uploaded_file.read().decode("utf-8").splitlines())
                        st.session_state['original_df'] = None
                        st.session_state['ip_column_name'] = None

                    if df_orig is not None:
                        st.session_state['original_df'] = df_orig
                        for col in df_orig.columns:
                            sample = df_orig[col].dropna().head(10).astype(str)
                            if any(is_valid_ip(val.strip()) for val in sample):
                                ip_col = col
                                break
                        
                        if ip_col:
                            st.session_state['ip_column_name'] = ip_col
                            raw_targets.extend(df_orig[ip_col].dropna().astype(str).tolist())
                            
                            # --- æ–°æ©Ÿèƒ½ï¼šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ---
                            st.info(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(df_orig)} è¡Œ / IPåˆ—: `{ip_col}`")
                            with st.expander("ğŸ‘€ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                                st.dataframe(df_orig)
                            # ---------------------------------------------
                        else:
                            st.error("ãƒ•ã‚¡ã‚¤ãƒ«å†…ã«IPã‚¢ãƒ‰ãƒ¬ã‚¹ã®åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

                except Exception as e:
                    st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # --- å…¬é–‹ãƒ¢ãƒ¼ãƒ‰æ™‚ã®ã¿ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è­¦å‘Šã‚’è¡¨ç¤º ---
    if IS_PUBLIC_MODE:
        st.warning("""
        **ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šã®æ³¨æ„**
        * **ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›æ¨å¥¨**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚ˆã‚Šã‚‚ã€å·¦å´ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã¸ã®**ã‚³ãƒ”ãƒ¼ï¼†ãƒšãƒ¼ã‚¹ãƒˆ**ã®æ–¹ãŒã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆä½œæˆè€…æƒ…å ±ãªã©ï¼‰ãŒå«ã¾ã‚Œãªã„ãŸã‚å®‰å…¨ã§ã™ã€‚
        * **ãƒ•ã‚¡ã‚¤ãƒ«åã«æ³¨æ„**: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«åã«æ©Ÿå¯†æƒ…å ±ï¼ˆä¾‹: `ClientA_Log.txt`ï¼‰ã‚’å«ã‚ãšã€`list.txt` ãªã©ã®ç„¡æ©Ÿè³ªãªåå‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        """)
    
    cleaned_raw_targets_list = []
    target_freq_counts = {}

    if raw_targets:
        cleaned_raw_targets_list = [clean_ocr_error_chars(t) for t in raw_targets]
        target_freq_counts = pd.Series(cleaned_raw_targets_list).value_counts().to_dict()
    else:
        target_freq_counts = {}

    targets = []
    ocr_error_chars = set('Iil|OoSsAaBâ…¡')

    for t in raw_targets:
        original_t = t
        is_ocr_error_likely = any(c in ocr_error_chars for c in original_t)
        if is_ocr_error_likely:
            cleaned_t = clean_ocr_error_chars(original_t)
            if is_valid_ip(cleaned_t):
                if cleaned_t not in targets: targets.append(cleaned_t)
                continue
            t = original_t
        
        invalid_ip_chars = set('ghijklmnopqrstuvwxyz')
        has_hyphen = '-' in t
        has_strictly_domain_char = any(c in invalid_ip_chars for c in t.lower())
        is_likely_domain_or_host = has_hyphen or has_strictly_domain_char
    
        if is_valid_ip(t):
            if t not in targets: targets.append(t)
        elif is_likely_domain_or_host:
            if t not in targets: targets.append(t)
        else:
            cleaned_t_final = clean_ocr_error_chars(t)
            if cleaned_t_final not in targets: targets.append(cleaned_t_final)

    has_new_targets = (targets != st.session_state.targets_cache)
    
    if has_new_targets or 'target_freq_map' not in st.session_state:
        st.session_state['target_freq_map'] = target_freq_counts
        st.session_state['original_input_list'] = cleaned_raw_targets_list
    ip_targets = [t for t in targets if is_valid_ip(t)]
    domain_targets = [t for t in targets if not is_valid_ip(t)]
    ipv6_count = sum(1 for t in ip_targets if not is_ipv4(t))
    ipv4_count = len(ip_targets) - ipv6_count

    st.markdown("---")
    st.markdown("### âš™ï¸ æ¤œç´¢è¡¨ç¤ºè¨­å®š")
    
    display_mode = st.radio(
        "**è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰:** (æ¤œç´¢çµæœã®è¡¨ç¤ºå½¢å¼ã¨APIä½¿ç”¨æœ‰ç„¡ã‚’è¨­å®š)",
        ("æ¨™æº–ãƒ¢ãƒ¼ãƒ‰", "é›†ç´„ãƒ¢ãƒ¼ãƒ‰ (IPv4 Group)", "ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰ (APIãªã—)"),
        key="display_mode_radio",
        horizontal=True
    )
    
    api_mode_selection = st.radio(
        "**API å‡¦ç†ãƒ¢ãƒ¼ãƒ‰:** (é€Ÿåº¦ã¨å®‰å®šæ€§ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•)",
        list(MODE_SETTINGS.keys()),
        key="api_mode_radio",
        horizontal=True
    )
    
    selected_settings = MODE_SETTINGS[api_mode_selection]
    max_workers = selected_settings["MAX_WORKERS"]
    delay_between_requests = selected_settings["DELAY_BETWEEN_REQUESTS"]
    rate_limit_wait_seconds = RATE_LIMIT_WAIT_SECONDS

    mode_mapping = {
        "æ¨™æº–ãƒ¢ãƒ¼ãƒ‰": "æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ (1ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ = 1è¡Œ)",
        "é›†ç´„ãƒ¢ãƒ¼ãƒ‰ (IPv4 Group)": "é›†ç´„ãƒ¢ãƒ¼ãƒ‰ (IPv4ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ISP/å›½åˆ¥ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–)",
        "ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰ (APIãªã—)": "ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰ (APIãªã— - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªãƒ³ã‚¯ã®ã¿)"
    }
    current_mode_full_text = mode_mapping[display_mode]

    st.markdown("---")
    col_act1, col_act2 = st.columns([3, 1])

    is_currently_searching = st.session_state.is_searching and not st.session_state.cancel_search
    
    total_ip_targets_for_display = len(ip_targets) + len(st.session_state.deferred_ips)

    with col_act1:
        st.success(f"**Target:** IPv4: {ipv4_count} / IPv6: {ipv6_count} / Domain: {len(domain_targets)} (Pending: {len(st.session_state.deferred_ips)}) / **CIDR Cache:** {len(st.session_state.cidr_cache)}")

    with col_act2:
        if is_currently_searching:
            if st.button("âŒ ä¸­æ­¢", type="secondary", use_container_width=True):
                st.session_state.cancel_search = True
                st.session_state.is_searching = False
                st.session_state.deferred_ips = {}
                st.rerun()
        else:
            execute_search = st.button(
            "ğŸš€ æ¤œç´¢é–‹å§‹",
            type="primary",
            use_container_width=True,
            disabled=(len(targets) == 0 and len(st.session_state.deferred_ips) == 0)
            )

    if ('execute_search' in locals() and execute_search and (has_new_targets or len(st.session_state.deferred_ips) > 0)) or is_currently_searching:
        
        if ('execute_search' in locals() and execute_search and has_new_targets and len(targets) > 0):
            st.session_state.is_searching = True
            st.session_state.cancel_search = False
            st.session_state.raw_results = []
            st.session_state.deferred_ips = {}
            st.session_state.finished_ips = set()
            st.session_state.targets_cache = targets
            st.session_state.search_start_time = time.time()
            st.rerun() 
            
        elif is_currently_searching:
            targets = st.session_state.targets_cache
            ip_targets = [t for t in targets if is_valid_ip(t)]
            domain_targets = [t for t in targets if not is_valid_ip(t)]

            st.subheader("â³ å‡¦ç†ä¸­...")
            
            total_targets = len(targets)
            total_ip_api_targets = len(ip_targets)
            
            ip_targets_to_process = [ip for ip in ip_targets if ip not in st.session_state.finished_ips]
            
            current_time = time.time()
            ready_to_retry_ips = []
            deferred_ips_new = {}
            for ip, defer_time in st.session_state.deferred_ips.items():
                if current_time >= defer_time:
                    ready_to_retry_ips.append(ip)
                else:
                    deferred_ips_new[ip] = defer_time
            
            st.session_state.deferred_ips = deferred_ips_new
            
            immediate_ip_queue_unique = []
            for ip in ip_targets_to_process:
                if ip not in st.session_state.deferred_ips and ip not in immediate_ip_queue_unique:
                    immediate_ip_queue_unique.append(ip)

            immediate_ip_queue = immediate_ip_queue_unique
            immediate_ip_queue.extend(ready_to_retry_ips)
            
            if "ç°¡æ˜“" in current_mode_full_text:
                if not st.session_state.raw_results:
                    results_list = []
                    for t in targets:
                        results_list.append(get_simple_mode_details(t))
                    st.session_state.raw_results = results_list
                    st.session_state.finished_ips.update(targets)
                    st.session_state.is_searching = False
                    st.rerun()

            else:
                if not any(res['ISP'] == 'Domain/Host' for res in st.session_state.raw_results) and domain_targets:
                    st.session_state.raw_results.extend([get_domain_details(d) for d in domain_targets])
                    st.session_state.finished_ips.update(domain_targets)
                    
                prog_bar_container = st.empty()
                status_text_container = st.empty()
                summary_container = st.empty() 

                if immediate_ip_queue:
                    cidr_cache_snapshot = st.session_state.cidr_cache.copy() 
                    
                    with ThreadPoolExecutor(max_workers=max_workers) as executor:
                        future_to_ip = {
                            executor.submit(
                                get_ip_details_from_api, 
                                ip, 
                                cidr_cache_snapshot, 
                                delay_between_requests, 
                                rate_limit_wait_seconds,
                                tor_nodes
                            ): ip for ip in immediate_ip_queue
                        }
                        remaining = set(future_to_ip.keys())
                        
                        while remaining and not st.session_state.cancel_search:
                            done, remaining = wait(remaining, timeout=0.1, return_when=FIRST_COMPLETED)
                            
                            for f in done:
                                res_tuple = f.result()
                                res = res_tuple[0]
                                new_cache_entry = res_tuple[1]
                                ip = res['Target_IP']
                                
                                if new_cache_entry:
                                    st.session_state.cidr_cache.update(new_cache_entry)
                                
                                if res.get('Status', '').startswith('Success'):
                                    st.session_state.raw_results.append(res)
                                    st.session_state.finished_ips.add(ip)
                                elif res.get('Defer_Until'):
                                    st.session_state.deferred_ips[ip] = res['Defer_Until']
                                else:
                                    st.session_state.raw_results.append(res)
                                    st.session_state.finished_ips.add(ip)

                            if total_ip_api_targets > 0:
                                processed_api_ips_count = len([ip for ip in st.session_state.finished_ips if is_valid_ip(ip)])
                                pct = int(processed_api_ips_count / total_ip_api_targets * 100)
                                elapsed_time = time.time() - st.session_state.search_start_time
                                eta_seconds = 0
                                if processed_api_ips_count > 0:
                                    rate = processed_api_ips_count / elapsed_time
                                    remaining_count = total_ip_api_targets - processed_api_ips_count
                                    eta_seconds = math.ceil(remaining_count / rate)
                                
                                eta_display = "è¨ˆç®—ä¸­..."
                                if eta_seconds > 0:
                                    minutes = int(eta_seconds // 60)
                                    seconds = int(eta_seconds % 60)
                                    eta_display = f"{minutes:02d}:{seconds:02d}"
                                    
                                with prog_bar_container:
                                    st.progress(pct)
                                with status_text_container:
                                    st.caption(f"**Progress:** {processed_api_ips_count}/{total_ip_api_targets} | **Deferred:** {len(st.session_state.deferred_ips)} | **CIDR Cache:** {len(st.session_state.cidr_cache)} | **Remaining Time:** {eta_display}")
                                
                                isp_df, country_df, freq_df, country_all_df, isp_full_df, country_full_df, freq_full_df = summarize_in_realtime(st.session_state.raw_results)
                                with summary_container.container():
                                    st.markdown("---")
                                    draw_summary_content(isp_df, country_df, freq_df, country_all_df, "ğŸ“Š Real-time analysis")
                                st.markdown("---")

                            if not remaining and not st.session_state.deferred_ips:
                                break
                            
                            if st.session_state.deferred_ips:
                                st.rerun()  
                            
                            time.sleep(0.5) 
                            
                        if total_ip_api_targets > 0 and not st.session_state.deferred_ips:
                            processed_api_ips_count = len([ip for ip in st.session_state.finished_ips if is_valid_ip(ip)])
                            final_pct = int(processed_api_ips_count / total_ip_api_targets * 100)
                            with prog_bar_container:
                                st.progress(final_pct)
                            with status_text_container:
                                st.caption(f"**Progress:** {processed_api_ips_count}/{total_ip_api_targets} | **Deferred:** {len(st.session_state.deferred_ips)} | **CIDR Cache:** {len(st.session_state.cidr_cache)} | **Remaining Time:** å®Œäº†")
                        
                if len(st.session_state.finished_ips) == total_targets and not st.session_state.deferred_ips:
                    st.session_state.is_searching = False
                    st.info("âœ… å…¨ã¦ã®æ¤œç´¢ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                    summary_container.empty()
                    st.rerun()
                
                elif st.session_state.deferred_ips and not st.session_state.cancel_search:
                    next_retry_time = min(st.session_state.deferred_ips.values())
                    wait_time = max(1, int(next_retry_time - time.time()))
                    
                    prog_bar_container.empty()
                    status_text_container.empty()
                    summary_container.empty()
                    st.warning(f"âš ï¸ **APIãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã«é”ã—ã¾ã—ãŸã€‚** éš”é›¢ä¸­ã® **{len(st.session_state.deferred_ips)}** ä»¶ã®IPã‚¢ãƒ‰ãƒ¬ã‚¹ã¯ **{wait_time}** ç§’å¾Œã«å†è©¦è¡Œã•ã‚Œã¾ã™ã€‚")
                    time.sleep(min(5, wait_time)) 
                    st.rerun()

                elif st.session_state.cancel_search:
                    prog_bar_container.empty()
                    status_text_container.empty()
                    summary_container.empty()
                    st.warning("æ¤œç´¢ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ­¢ã•ã‚Œã¾ã—ãŸã€‚")
                    st.session_state.is_searching = False
                    st.rerun()


    # --- çµæœè¡¨ç¤º ---
    if st.session_state.raw_results or st.session_state.deferred_ips:
        res = st.session_state.raw_results
        
        if st.session_state.get('debug_summary'):
            with st.expander("ğŸ› ï¸ ãƒ‡ãƒãƒƒã‚°æƒ…å ± (é›†è¨ˆãƒ‡ãƒ¼ã‚¿ç¢ºèªç”¨)", expanded=False):
                st.markdown("**API å‡¦ç†ãƒ¢ãƒ¼ãƒ‰è¨­å®š**")
                st.write(f"MAX_WORKERS: {max_workers}")
                st.write(f"DELAY_BETWEEN_REQUESTS: {delay_between_requests}")
                st.markdown("---")
                st.json(st.session_state['debug_summary'].get('country_code_counts', {}))
                st.json(st.session_state['debug_summary'].get('country_all_df', []))
                st.markdown("---")
                st.json(st.session_state.get('cidr_cache', {}))

        
        successful_results = [r for r in res if r['Status'].startswith('Success') or r['Status'].startswith('Aggregated')]
        error_results = [r for r in res if not (r['Status'].startswith('Success') or r['Status'].startswith('Aggregated'))]
        
        for ip, defer_time in st.session_state.deferred_ips.items():
            status = f"Pending (Retry in {max(0, int(defer_time - time.time()))}s)"
            error_results.append({
                'Target_IP': ip, 'ISP': 'N/A', 'Country': 'N/A', 'CountryCode': 'N/A', 'RIR_Link': get_authoritative_rir_link(ip, 'N/A'),
                'Secondary_Security_Links': create_secondary_links(ip), 
                'Status': status
            })
        
        if "é›†ç´„" in current_mode_full_text:
            display_res = group_results_by_isp(successful_results)
            display_res.extend(error_results)
        else:
            display_res = successful_results + error_results
            target_order = {ip: i for i, ip in enumerate(targets)}
            display_res.sort(key=lambda x: target_order.get(get_copy_target(x['Target_IP']), float('inf')))

        display_results(display_res, current_mode_full_text, display_mode)
        
        if not st.session_state.is_searching or st.session_state.cancel_search:
            isp_df, country_df, freq_df, country_all_df, isp_full_df, country_full_df, freq_full_df = summarize_in_realtime(st.session_state.raw_results)
            
            st.markdown("---")
            draw_summary_content(isp_df, country_df, freq_df, country_all_df, "âœ… é›†è¨ˆçµæœ")

            # --- å…ƒãƒ‡ãƒ¼ã‚¿çµåˆå‡¦ç†ï¼ˆç”»é¢è¡¨ç¤º & ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…±é€šï¼‰ ---
            df_with_res = pd.DataFrame() # åˆæœŸåŒ–
            if st.session_state.get('original_df') is not None and st.session_state.get('ip_column_name'):
                df_with_res = st.session_state['original_df'].copy()
                ip_col = st.session_state['ip_column_name']
                results = st.session_state.get('raw_results', []) 
                
                if results:
                    res_dict = {r['Target_IP']: r for r in results}

                    # å„è¡Œã®IPã«åŸºã¥ã„ã¦çµæœã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
                    isps, isps_jp, countries, countries_jp, proxy_type, statuses = [], [], [], [], [], []
                    for ip_val in df_with_res[ip_col]:
                        ip_val_str = str(ip_val).strip()
                        info = res_dict.get(ip_val_str, {})
                        isps.append(info.get('ISP', 'N/A'))
                        isps_jp.append(info.get('ISP_JP', 'N/A')) 
                        countries.append(info.get('Country', 'N/A'))
                        countries_jp.append(info.get('Country_JP', 'N/A'))
                        proxy_type.append(info.get('Proxy_Type', ''))
                        statuses.append(info.get('Status', 'N/A'))
                    
                    # çµåˆ
                    insert_idx = df_with_res.columns.get_loc(ip_col) + 1
                    df_with_res.insert(insert_idx, 'Status', statuses)
                    df_with_res.insert(insert_idx, 'Proxy Type', proxy_type)
                    df_with_res.insert(insert_idx, 'Country_JP', countries_jp)
                    df_with_res.insert(insert_idx, 'Country', countries)
                    df_with_res.insert(insert_idx, 'ISP_JP', isps_jp)
                    df_with_res.insert(insert_idx, 'ISP', isps)

            # --- æ–°æ©Ÿèƒ½ï¼šå…ƒãƒ‡ãƒ¼ã‚¿ x æ¤œç´¢çµæœ ã‚¯ãƒ­ã‚¹åˆ†æè¡¨ç¤º ---
            if not df_with_res.empty:
                st.markdown("---")
                render_merged_analysis(df_with_res)
            # ------------------------------------------------

            # --- å…¨ä»¶é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
            st.markdown("### ğŸ“Š é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨ç‰ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            st.caption("â€» ä¸Šè¨˜ã‚°ãƒ©ãƒ•ã®Top10åˆ¶é™ã‚’è§£é™¤ã—ãŸã€ã™ã¹ã¦ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã¨ã‚°ãƒ©ãƒ•ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
            
            col_full_dl1, col_full_dl2, col_full_dl3, col_full_dl4 = st.columns(4)
            
            with col_full_dl1:
                st.download_button(
                    "â¬‡ï¸ å¯¾è±¡IP ã‚«ã‚¦ãƒ³ãƒˆ (å…¨ä»¶)",
                    freq_full_df.to_csv(index=False).encode('utf-8-sig'),
                    "target_ip_frequency_all.csv",
                    "text/csv",
                    use_container_width=True
                )
            with col_full_dl2:
                st.download_button(
                    "â¬‡ï¸ ISPåˆ¥ ã‚«ã‚¦ãƒ³ãƒˆ (å…¨ä»¶)",
                    isp_full_df.to_csv(index=False).encode('utf-8-sig'),
                    "isp_counts_all.csv",
                    "text/csv",
                    use_container_width=True
                )
            with col_full_dl3:
                st.download_button(
                    "â¬‡ï¸ å›½åˆ¥ ã‚«ã‚¦ãƒ³ãƒˆ (å…¨ä»¶)",
                    country_full_df.to_csv(index=False).encode('utf-8-sig'),
                    "country_counts_all.csv",
                    "text/csv",
                    use_container_width=True
                )
            
            with col_full_dl4:
                # å…¨ä»¶ã‚°ãƒ©ãƒ•HTMLãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
                html_report = generate_full_report_html(isp_full_df, country_full_df, freq_full_df)
                st.download_button(
                    "â¬‡ï¸ å…¨ä»¶ã‚°ãƒ©ãƒ•HTMLãƒ¬ãƒãƒ¼ãƒˆ",
                    html_report,
                    "whois_analysis_report.html",
                    "text/html",
                    use_container_width=True
                )

        
        st.markdown("### â¬‡ï¸ æ¤œç´¢çµæœãƒªã‚¹ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        col_dl1, col_dl2, col_dl3 = st.columns(3)
        # 1. ç”»é¢è¡¨ç¤ºé †ãƒ‡ãƒ¼ã‚¿
        csv_display = pd.DataFrame(display_res).drop(columns=['CountryCode', 'Secondary_Security_Links', 'RIR_Link'], errors='ignore').astype(str)
        with col_dl1:
            st.download_button("â¬‡ï¸ CSV (ç”»é¢è¡¨ç¤ºé †)", csv_display.to_csv(index=False).encode('utf-8-sig'), "whois_results_display.csv", "text/csv", use_container_width=True)
            # Excel (Display)
            excel_display = convert_df_to_excel(csv_display)
            st.download_button("â¬‡ï¸ Excel (ç”»é¢è¡¨ç¤ºé †)", excel_display, "whois_results_display.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", use_container_width=True)

        # 2. å…¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¥åŠ›é †ï¼‰
        result_lookup = {r['Target_IP']: r for r in st.session_state.raw_results}
        full_output_data = []
        for original_t in st.session_state.get('original_input_list', []):
            if original_t in result_lookup:
                full_output_data.append(result_lookup[original_t])
            else:
                full_output_data.append({'Target_IP': original_t, 'ISP': 'N/A', 'ISP_JP': 'N/A', 'Country': 'N/A', 'Country_JP': 'N/A', 'Status': 'Pending/Error'})
        
        csv_full = pd.DataFrame(full_output_data).drop(columns=['CountryCode', 'Secondary_Security_Links', 'RIR_Link'], errors='ignore').astype(str)
        with col_dl2:
            st.download_button("â¬‡ï¸ CSV (å…¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿é †)", csv_full.to_csv(index=False).encode('utf-8-sig'), "whois_results_full.csv", "text/csv", use_container_width=True)
            # Excel (Full)
            excel_full = convert_df_to_excel(csv_full)
            st.download_button("â¬‡ï¸ Excel (å…¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿é †)", excel_full, "whois_results_full.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", use_container_width=True)

        with col_dl3:
            # 3. å…ƒãƒ‡ãƒ¼ã‚¿çµåˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå…±é€šå‡¦ç†ã§ä½œæˆæ¸ˆã¿ã®df_with_resã‚’ä½¿ç”¨ï¼‰
            if not IS_PUBLIC_MODE and not df_with_res.empty:
                st.markdown("**ğŸ” åˆ†æä»˜ãExcel (Pivot/Graph)**")
                
                # æ™‚é–“å¸¯åˆ†æç”¨ã®åˆ—é¸æŠãƒœãƒƒã‚¯ã‚¹
                time_cols = [c for c in df_with_res.columns if 'date' in c.lower() or 'time' in c.lower() or 'jst' in c.lower()]
                default_idx = df_with_res.columns.get_loc(time_cols[0]) if time_cols else 0
                
                selected_time_col = st.selectbox(
                    "æ™‚é–“å¸¯åˆ†æ(Houråˆ—)ã«ä½¿ã†æ—¥æ™‚åˆ—ã‚’é¸æŠ:", 
                    df_with_res.columns, 
                    index=default_idx,
                    key="time_col_selector"
                )

                # Advanced Excelç”Ÿæˆ (v5.0)
                excel_advanced = create_advanced_excel(df_with_res, selected_time_col)
                
                st.download_button(
                    "â¬‡ï¸ Excel (åˆ†æãƒ»ã‚°ãƒ©ãƒ•ä»˜ã)", 
                    excel_advanced, 
                    "whois_analysis_master.xlsx", 
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", 
                    use_container_width=True,
                    help="ç”Ÿãƒ‡ãƒ¼ã‚¿ã«åŠ ãˆã€ISPåˆ¥ãƒ»æ™‚é–“å¸¯åˆ¥ã®é›†è¨ˆè¡¨ã¨ã‚°ãƒ©ãƒ•ï¼ˆãƒ”ãƒœãƒƒãƒˆï¼‰ãŒåˆ¥ã‚·ãƒ¼ãƒˆã«å«ã¾ã‚Œã¾ã™ã€‚"
                )
            else:
                st.button("â¬‡ï¸ Excel (CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã®ã¿)", disabled=True, use_container_width=True)

if __name__ == "__main__":
    main()
